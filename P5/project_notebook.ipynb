{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Project Overview\n",
    "In this project, you will play detective, and put your machine learning skills to use by building an algorithm to identify Enron Employees who may have committed fraud based on the public Enron financial and email dataset.\n",
    "\n",
    "Prepare for this project with: Intro to Machine Learning.\n",
    "\n",
    "Note\n",
    "If you have successfully completed the project for the Intro to Machine Learning course in the past (which entails having graduated from the course and having access to your course certificate), simply email us at dataanalyst-project@udacity.com with your passing evaluation and we'll give you credit for this project.\n",
    "\n",
    "Why this Project?\n",
    "This project will teach you the end-to-end process of investigating data through a machine learning lens.\n",
    "\n",
    "It will teach you how to extract and identify useful features that best represent your data, a few of the most commonly used machine learning algorithms today, and how to evaluate the performance of your machine learning algorithms.\n",
    "\n",
    "What will I learn?\n",
    "By the end of the project, you will be able to:\n",
    "\n",
    "Deal with an imperfect, real-world dataset\n",
    "Validate a machine learning result using test data\n",
    "Evaluate a machine learning result using quantitative metrics\n",
    "Create, select and transform features\n",
    "Compare the performance of machine learning algorithms\n",
    "Tune machine learning algorithms for maximum performance\n",
    "Communicate your machine learning algorithm results clearly\n",
    "Why is this Important to my Career?\n",
    "Machine learning is a first-class ticket to the most exciting careers in data analysis today.\n",
    "\n",
    "As data sources proliferate along with the computing power to process them, going straight to the data is one of the most straightforward ways to quickly gain insights and make predictions.\n",
    "\n",
    "Machine learning brings together computer science and statistics to harness that predictive power."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "How do I Complete this Project?\n",
    "This project is connected to the Intro to Machine Learning course, but depending on your background knowledge of machine learning, you may not need to take the whole thing to complete this project.\n",
    "\n",
    "A note before you begin: the mini-projects in the Intro to Machine Learning class were mostly designed to have lots of data points, give intuitive results, and otherwise behave nicely. This project is significantly tougher in that we're now using the real data, which can be messy and does not have as many data points as we usually hope for when doing machine learning. Don't get discouraged--imperfect data is something you need to be used to as a data analyst! If you encounter something you haven't seen before, take a step back and think about a smart way around. You can do it!\n",
    "\n",
    "Project Overview\n",
    "In 2000, Enron was one of the largest companies in the United States. By 2002, it had collapsed into bankruptcy due to widespread corporate fraud. In the resulting Federal investigation, a significant amount of typically confidential information entered into the public record, including tens of thousands of emails and detailed financial data for top executives. In this project, you will play detective, and put your new skills to use by building a person of interest identifier based on financial and email data made public as a result of the Enron scandal. To assist you in your detective work, we've combined this data with a hand-generated list of persons of interest in the fraud case, which means individuals who were indicted, reached a settlement or plea deal with the government, or testified in exchange for prosecution immunity.\n",
    "\n",
    "Resources Needed\n",
    "You should have python and sklearn running on your computer, as well as the starter code (both python scripts and the Enron dataset) that you downloaded as part of the first mini-project in the Intro to Machine Learning course. You can get the starter code on git: git clone https://github.com/udacity/ud120-projects.git\n",
    "\n",
    "The starter code can be found in the final_project directory of the codebase that you downloaded for use with the mini-projects. Some relevant files: \n",
    "\n",
    "poi_id.py : Starter code for the POI identifier, you will write your analysis here. You will also submit a version of this file for your evaluator to verify your algorithm and results. \n",
    "\n",
    "final_project_dataset.pkl : The dataset for the project, more details below. \n",
    "\n",
    "tester.py : When you turn in your analysis for evaluation by Udacity, you will submit the algorithm, dataset and list of features that you use (these are created automatically in poi_id.py). The evaluator will then use this code to test your result, to make sure we see performance that’s similar to what you report. You don’t need to do anything with this code, but we provide it for transparency and for your reference. \n",
    "\n",
    "emails_by_address : this directory contains many text files, each of which contains all the messages to or from a particular email address. It is for your reference, if you want to create more advanced features based on the details of the emails dataset. You do not need to process the e-mail corpus in order to complete the project.\n",
    "\n",
    "Steps to Success\n",
    "We will provide you with starter code that reads in the data, takes your features of choice, then puts them into a numpy array, which is the input form that most sklearn functions assume. Your job is to engineer the features, pick and tune an algorithm, and to test and evaluate your identifier. Several of the mini-projects were designed with this final project in mind, so be on the lookout for ways to use the work you’ve already done.\n",
    "\n",
    "As preprocessing to this project, we've combined the Enron email and financial data into a dictionary, where each key-value pair in the dictionary corresponds to one person. The dictionary key is the person's name, and the value is another dictionary, which contains the names of all the features and their values for that person. The features in the data fall into three major types, namely financial features, email features and POI labels.\n",
    "\n",
    "financial features: ['salary', 'deferral_payments', 'total_payments', 'loan_advances', 'bonus', 'restricted_stock_deferred', 'deferred_income', 'total_stock_value', 'expenses', 'exercised_stock_options', 'other', 'long_term_incentive', 'restricted_stock', 'director_fees'] (all units are in US dollars)\n",
    "\n",
    "email features: ['to_messages', 'email_address', 'from_poi_to_this_person', 'from_messages', 'from_this_person_to_poi', 'shared_receipt_with_poi'] (units are generally number of emails messages; notable exception is ‘email_address’, which is a text string)\n",
    "\n",
    "POI label: [‘poi’] (boolean, represented as integer)\n",
    "\n",
    "You are encouraged to make, transform or rescale new features from the starter features. If you do this, you should store the new feature to my_dataset, and if you use the new feature in the final algorithm, you should also add the feature name to my_feature_list, so your evaluator can access it during testing. For a concrete example of a new feature that you could add to the dataset, refer to the lesson on Feature Selection.\n",
    "\n",
    "In addition, we advise that you keep notes as you work through the project. As part of your project submission, you will compose answers to a series of questions (also given on the next page) to understand your approach towards different aspects of the analysis. Your thought process is, in many ways, more important than your final project and we will by trying to probe your thought process in these questions."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Enron Submission Free-Response Questions\n",
    "A critical part of machine learning is making sense of your analysis process and communicating it to others. The questions below will help us understand your decision-making process and allow us to give feedback on your project. Please answer each question; your answers should be about 1-2 paragraphs per question. If you find yourself writing much more than that, take a step back and see if you can simplify your response!\n",
    "When your evaluator looks at your responses, he or she will use a specific list of rubric items to assess your answers. Here is the link to that rubric: [Link] Each question has one or more specific rubric items associated with it, so before you submit an answer, take a look at that part of the rubric. If your response does not meet expectations for all rubric points, you will be asked to revise and resubmit your project. Make sure that your responses are detailed enough that the evaluator will be able to understand the steps you took and your thought processes as you went through the data analysis.\n",
    "Once you’ve submitted your responses, your coach will take a look and may ask a few more focused follow-up questions on one or more of your answers.  \n",
    "We can’t wait to see what you’ve put together for this project!\n",
    "Summarize for us the goal of this project and how machine learning is useful in trying to accomplish it. As part of your answer, give some background on the dataset and how it can be used to answer the project question. Were there any outliers in the data when you got it, and how did you handle those?  [relevant rubric items: “data exploration”, “outlier investigation”]\n",
    "What features did you end up using in your POI identifier, and what selection process did you use to pick them? Did you have to do any scaling? Why or why not? As part of the assignment, you should attempt to engineer your own feature that does not come ready-made in the dataset -- explain what feature you tried to make, and the rationale behind it. (You do not necessarily have to use it in the final analysis, only engineer and test it.) In your feature selection step, if you used an algorithm like a decision tree, please also give the feature importances of the features that you use, and if you used an automated feature selection function like SelectKBest, please report the feature scores and reasons for your choice of parameter values.  [relevant rubric items: “create new features”, “properly scale features”, “intelligently select feature”]\n",
    "What algorithm did you end up using? What other one(s) did you try? How did model performance differ between algorithms?  [relevant rubric item: “pick an algorithm”]\n",
    "What does it mean to tune the parameters of an algorithm, and what can happen if you don’t do this well?  How did you tune the parameters of your particular algorithm? What parameters did you tune? (Some algorithms do not have parameters that you need to tune -- if this is the case for the one you picked, identify and briefly explain how you would have done it for the model that was not your final choice or a different model that does utilize parameter tuning, e.g. a decision tree classifier).  [relevant rubric item: “tune the algorithm”]\n",
    "What is validation, and what’s a classic mistake you can make if you do it wrong? How did you validate your analysis?  [relevant rubric item: “validation strategy”]\n",
    "Give at least 2 evaluation metrics and your average performance for each of them.  Explain an interpretation of your metrics that says something human-understandable about your algorithm’s performance. [relevant rubric item: “usage of evaluation metrics”]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Udacity Logo\n",
    "Logout\n",
    "PROJECT SPECIFICATION\n",
    "Identify Fraud from Enron Email\n",
    "\n",
    "Quality of Code\n",
    "\n",
    "CRITERIA\n",
    "MEETS SPECIFICATIONS\n",
    "Functionality\n",
    "\n",
    "Code reflects the description in the answers to questions in the writeup. i.e. code performs the functions documented in the writeup and the writeup clearly specifies the final analysis strategy.\n",
    "\n",
    "Usability\n",
    "\n",
    "poi_id.py can be run to export the dataset, list of features and algorithm, so that the final algorithm can be checked easily using tester.py.\n",
    "\n",
    "Understanding the Dataset and Question\n",
    "\n",
    "CRITERIA\n",
    "MEETS SPECIFICATIONS\n",
    "Data Exploration (related mini-project: Lesson 5)\n",
    "\n",
    "Student response addresses the most important characteristics of the dataset and uses these characteristics to inform their analysis. Important characteristics include:\n",
    "\n",
    "total number of data points\n",
    "allocation across classes (POI/non-POI)\n",
    "number of features used\n",
    "are there features with many missing values? etc.\n",
    "Outlier Investigation (related mini-project: Lesson 7)\n",
    "\n",
    "Student response identifies outlier(s) in the financial data, and explains how they are removed or otherwise handled.\n",
    "\n",
    "Optimize Feature Selection/Engineering\n",
    "\n",
    "CRITERIA\n",
    "MEETS SPECIFICATIONS\n",
    "Create new features (related mini-project: Lesson 11)\n",
    "\n",
    "At least one new feature is implemented. Justification for that feature is provided in the written response. The effect of that feature on final algorithm performance is tested or its strength is compared to other features in feature selection. The student is not required to include their new feature in their final feature set.\n",
    "\n",
    "Intelligently select features (related mini-project: Lesson 11)\n",
    "\n",
    "Univariate or recursive feature selection is deployed, or features are selected by hand (different combinations of features are attempted, and the performance is documented for each one). Features that are selected are reported and the number of features selected is justified. For an algorithm that supports getting the feature importances (e.g. decision tree) or feature scores (e.g. SelectKBest), those are documented as well.\n",
    "\n",
    "Properly scale features (related mini-project: Lesson 9)\n",
    "\n",
    "If algorithm calls for scaled features, feature scaling is deployed.\n",
    "\n",
    "Pick and Tune an Algorithm\n",
    "\n",
    "CRITERIA\n",
    "MEETS SPECIFICATIONS\n",
    "Pick an algorithm (related mini-project: Lessons 1-3)\n",
    "\n",
    "At least two different algorithms are attempted and their performance is compared, with the best performing one used in the final analysis.\n",
    "\n",
    "Discuss parameter tuning and its importance.\n",
    "\n",
    "Response addresses what it means to perform parameter tuning and why it is important.\n",
    "\n",
    "Tune the algorithm (related mini-project: Lessons 2, 3, 13)\n",
    "\n",
    "At least one important parameter tuned with at least 3 settings investigated systematically, or any of the following are true:\n",
    "\n",
    "GridSearchCV used for parameter tuning\n",
    "Several parameters tuned\n",
    "Parameter tuning incorporated into algorithm selection (i.e. parameters tuned for more than one algorithm, and best algorithm-tune combination selected for final analysis).\n",
    "Validate and Evaluate\n",
    "\n",
    "CRITERIA\n",
    "MEETS SPECIFICATIONS\n",
    "Usage of Evaluation Metrics (related mini-project: Lesson 14)\n",
    "\n",
    "At least two appropriate metrics are used to evaluate algorithm performance (e.g. precision and recall), and the student articulates what those metrics measure in context of the project task.\n",
    "\n",
    "Discuss validation and its importance.\n",
    "\n",
    "Response addresses what validation is and why it is important.\n",
    "\n",
    "Validation Strategy (related mini-project: Lesson 13)\n",
    "\n",
    "Performance of the final algorithm selected is assessed by splitting the data into training and testing sets or through the use of cross validation, noting the specific type of validation performed.\n",
    "\n",
    "Algorithm Performance\n",
    "\n",
    "When tester.py is used to evaluate performance, precision and recall are both at least 0.3.\n",
    "\n",
    "Student FAQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enron Submission Free-Response Questions\n",
    "A critical part of machine learning is making sense of your analysis process and communicating it to others. The questions below will help us understand your decision-making process and allow us to give feedback on your project. Please answer each question; your answers should be about 1-2 paragraphs per question. If you find yourself writing much more than that, take a step back and see if you can simplify your response!\n",
    "When your evaluator looks at your responses, he or she will use a specific list of rubric items to assess your answers. Here is the link to that rubric: [Link] Each question has one or more specific rubric items associated with it, so before you submit an answer, take a look at that part of the rubric. If your response does not meet expectations for all rubric points, you will be asked to revise and resubmit your project. Make sure that your responses are detailed enough that the evaluator will be able to understand the steps you took and your thought processes as you went through the data analysis.\n",
    "Once you’ve submitted your responses, your coach will take a look and may ask a few more focused follow-up questions on one or more of your answers.  \n",
    "We can’t wait to see what you’ve put together for this project!\n",
    "Summarize for us the goal of this project and how machine learning is useful in trying to accomplish it. As part of your answer, give some background on the dataset and how it can be used to answer the project question. Were there any outliers in the data when you got it, and how did you handle those?  [relevant rubric items: “data exploration”, “outlier investigation”]\n",
    "What features did you end up using in your POI identifier, and what selection process did you use to pick them? Did you have to do any scaling? Why or why not? As part of the assignment, you should attempt to engineer your own feature that does not come ready-made in the dataset -- explain what feature you tried to make, and the rationale behind it. (You do not necessarily have to use it in the final analysis, only engineer and test it.) In your feature selection step, if you used an algorithm like a decision tree, please also give the feature importances of the features that you use, and if you used an automated feature selection function like SelectKBest, please report the feature scores and reasons for your choice of parameter values.  [relevant rubric items: “create new features”, “properly scale features”, “intelligently select feature”]\n",
    "What algorithm did you end up using? What other one(s) did you try? How did model performance differ between algorithms?  [relevant rubric item: “pick an algorithm”]\n",
    "What does it mean to tune the parameters of an algorithm, and what can happen if you don’t do this well?  How did you tune the parameters of your particular algorithm? What parameters did you tune? (Some algorithms do not have parameters that you need to tune -- if this is the case for the one you picked, identify and briefly explain how you would have done it for the model that was not your final choice or a different model that does utilize parameter tuning, e.g. a decision tree classifier).  [relevant rubric item: “tune the algorithm”]\n",
    "What is validation, and what’s a classic mistake you can make if you do it wrong? How did you validate your analysis?  [relevant rubric item: “validation strategy”]\n",
    "Give at least 2 evaluation metrics and your average performance for each of them.  Explain an interpretation of your metrics that says something human-understandable about your algorithm’s performance. [relevant rubric item: “usage of evaluation metrics”]\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Project Submission\n",
    "\n",
    "Play detective and put your machine learning skills to use by building an algorithm to identify Enron Employees who may have committed fraud based on the public Enron financial and email dataset.\n",
    "\n",
    "General Submission and Evaluation Overview\n",
    "Your submission will contain several files: the code/classifier you create and some written documentation of your work. We will evaluate your project according to the rubric here; only projects that satisfy all \"meets expectations\" items will pass. Please self-evaluate before you submit! If you don't think your project meets all the criteria, the project evaluator likely won't either.\n",
    "\n",
    "Submission\n",
    "Ready to submit your project? Go back to your Udacity Home, click on the project, and follow the instructions to submit!\n",
    "\n",
    "You can either send us a GitHub link of the files or upload a compressed directory (zip file).\n",
    "Inside the zip folder include a text file with a list of Web sites, books, forums, blog posts, GitHub repositories etc that you referred to or used in this submission (Add N/A if you did not use such resources).\n",
    "It can take us up to a week to grade the project, but in most cases it is much faster. You will receive an email when your submission has been reviewed.\n",
    "\n",
    "If you are having any problems submitting your project or wish to check on the status of your submission, please email us at dataanalyst-project@udacity.com.\n",
    "\n",
    "Items to include in submission:\n",
    "Code/Classifier\n",
    "When making your classifier, you will create three pickle files (my_dataset.pkl, my_classifier.pkl, my_feature_list.pkl). The project evaluator will test these using the tester.py script. You are encouraged to use this script before submitting to gauge if your performance is good enough. You should also include your modified poi_id.py file in case of any issues with running your code or to verify what is reported in your question responses (see next paragraph). Notably, we should be able to run poi_id.py to generate the three pickle files that reflect your final algorithm, without needing to modify the script in any way.\n",
    "\n",
    "If you have intermediate code that you would like to provide as supplemental materials, it is encouraged for you to save them in files separate from poi_id.py. If you do so, be sure to provide a readme file that explains what each file is for. If you used a Jupyter notebook to work on the project, make sure that your finished code is transferred to the poi_id.py script to generate your final work.\n",
    "\n",
    "Documentation of Your Work\n",
    "Document the work you've done by answering (in about one or two paragraphs each) the questions found here. You can write your answers in a PDF, text/markdown file, HTML, or similar format. The responses in your documentation should allow a reviewer to understand and follow the steps you took in your project and to verify your understanding of the methods you have performed.\n",
    "\n",
    "Text File Listing Your References\n",
    "A list of Web sites, books, forums, blog posts, github repositories etc. that you referred to or used in this submission (add N/A if you did not use such resources). Please carefully read the following statement and include it in your document “I hereby confirm that this submission is my work. I have cited above the origins of any parts of the submission that were taken from Websites, books, forums, blog posts, github repositories, etc.\n",
    "\n",
    "Good Luck!\n",
    "You have not submitted the project yetSUBMIT PROJECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Order:\n",
    "Welcome to machine learning\n",
    "Naive Bayes\n",
    "SVM\n",
    "Decision Trees\n",
    "Choose your own algorithm\n",
    "Datasets and Questions\n",
    "Regressions\n",
    "Outliers\n",
    "Clustering\n",
    "Feature Scaling\n",
    "Text Learning\n",
    "Feature Selection\n",
    "PCA\n",
    "Validation\n",
    "Evaluation metrics\n",
    "Tying it all together\n",
    "Project\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
