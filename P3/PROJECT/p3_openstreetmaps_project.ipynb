{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse and clean OpenStreetMap data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify, archive, document data file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data File: New Orleans [MapZen Extract](https://mapzen.com/data/metro-extracts/metro/new-orleans_louisiana/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect and document general structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essential task is to parse data file and clean. What am I parsing?\n",
    "\n",
    "First, looking at the documentation, the gist that I get is:\n",
    "* node is a lon/lat point, possibly with 'tag' child elements\n",
    "* way has nodes as children(as nd elements) and defines things like roads, buildings, etc. Also may have 'tag' children\n",
    "* relation has nested 'member' elements which refer to ways and nodes - defines relationships among elements\n",
    "\n",
    "Next, open doc in vim - get aquanted, why? Because I can and I want to. Visually I see (screenshots):\n",
    "* standard opening:  <?xml version='1.0' encoding='UTF-8'?>\n",
    "* root is an 'osm' tag\n",
    "* doc is 16,082,009 lines long in total - wow!\n",
    "* skipping down some pages, looks like a bunch of nodes with no internal element defined at top\n",
    "* skipping down from middle of doc, looks like a bunch of 'way' tags of natural features (streams, other waterway)\n",
    "* skipping up from the end of the doc, looks like a bunch of 'relations' of things like gardens and public transportation stations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speculate on what to work with specifically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what should I clean here? 'Tag' elements seem to be an important part where details about map features are stored. This should also be where a lot of user-generated/added content is stored.\n",
    "\n",
    "In the middle, what is NHD? Strikes me as interesting. So Googling it, Ah, US Geological Survey's National Hydrography Dataset (Watershed Boundary Dataset). The data should be damn near perfect. Simply, it should be a way, with node child element references and tags with all relevant info (so  poly boundary and some meta data). These are mostly 'natural' features - auditing them sounds interesting.\n",
    "\n",
    "First, the NHD data shouldn't really have many errors given the data is taken from the NHD and uploaded to OpenStreetMaps (as opposed to multiple user-generated content for most contributor use-cases of OpenStreetMap). It's worth checking a few things to be sure. Some inspection is in order:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect/audit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, what keys exist in natural feature tag elements as a whole?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "OSM_FILE = \"/Users/mchana/GitHub/udacity/large_files/new-orleans_region.osm\"\n",
    "\n",
    "def count_elem(osm_file):\n",
    "    tag_set = set() \n",
    "    \n",
    "    context = ET.iterparse(osm_file, events=('start', 'end'))\n",
    "    _, root = next(context)\n",
    "    \n",
    "    for event, elem in context:\n",
    "        if event == 'end' and (elem.tag == 'node' or elem.tag == 'way' or elem.tag == 'relation'):\n",
    "            for tag_elem in elem.iter(\"tag\"):\n",
    "                if tag_elem.attrib['k'] == 'natural':\n",
    "                    for tag_elem in elem.iter(\"tag\"):\n",
    "                        tag_set.add(tag_elem.attrib['k'])\n",
    "        root.clear()\n",
    "    return tag_set\n",
    "\n",
    "keys_list = list(count_elem(OSM_FILE))\n",
    "print(keys_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hard to tell visually - sort it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted(keys_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the k values like 'golf' seem pretty arbitrary, but they may have a purpose. Others, like NHD:FCode seem redundant with FCode. Maybe US Geological society changed their naming convention at one point. There may be something to clean here... I also see a 'fixme' key which may be something. Worth noting for now, but what will really help is to create a dictionary of the features and their attribute contents.\n",
    "\n",
    "Some notes:\n",
    "* FCODE is used at least once - and if it's more, all of them use '46600' as their contents. Jumping down to NHD:FCode I see more contents and again, the '46600'. I'm betting FCODE and NHD:FCode are the same. Looking at the NHD poster I find that 46600 is for \"Swamp/Marsh\". The different keys here are definitely trying to achieve the same thing. They can be merged. Also, I'm not worried about a key having multiple FCode - for example 33600;46600 is a Canal/Ditch, but it's also a Swamp/Marsh. \n",
    "* Next redundant tag key is FDATE. 'FDATE' has 'Mon May 23 00:00:00 CEST 2011'as its contents. Looks like a time stamp - checking. I'll check to make sure other elements with the FDATE tag elem have a different timestamp. I could do this, but there's a better - faster way (all I need to do is see a few others to verify it's a time stamp. Vim - /search_key for 'FDATE\". Sure enough, found on line 13,356,160. Vim - n finds no other instances! What about NHD:Fdate? Yes, all kinds of others, but their dates ar emore simple - for example, \"2005/12/05\". Fair enough to say that the rogue element's date can be changed to 2011/05/23. \n",
    "* Now thinking, there only seems to be a single rogue elem. Inspect it to make sure it's not wildly different. Nothing stands out - /search for the user Aleks-Berlin. He also seems to have edited a few others - some tags called \"FIXME\", which seem a little haphazard. I'm wondering who this person is and if entering data haphazardly. Check element on map: /search for one of the nodes, then enter lat/lon coordinates on map: 29.3557244, -90.0538116. Looks like a valid natural feature, so the way elem should stay in place. \n",
    "* Next, FTYPE. 466 corresponds to Feature Type from pdf. \n",
    "* 'natural' all good\n",
    "* 'PERMANENT_ is redundant. Has strange value: &#123;A0AFF249-A7D2-44F4-AD8A-0A4A68F99450&#125;\n",
    "    * Search for NHD:Permanent_. They all have much simpler values like \"151098380\". What is it? On PDF there is a 'Permanent_Identifier' feature that is a 40 character string, but all the ones I find are 9 characters. Not enough info here - so leaving this one alone. Somehow can flag this elem?\n",
    "*RESOLUTION is '2'. From NHD documentation, this should be set to \"High\" (Code of source resolution: 1=Local resolution, 2=High resolution, 3=Medium Resolution.)\n",
    "* SHAPE_AREA and SHAPE_LENG are strange keys not found in any other elements. Again, wondering if best to just remove this element!\n",
    "    \n",
    "Now onto auditing some some of the other keys. For this it's best to build the dictionary and see what kind of information they hold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import collections as col\n",
    "import pprint\n",
    "\n",
    "OSM_FILE = \"/Users/mchana/GitHub/udacity/large_files/new-orleans_region.osm\"\n",
    "\n",
    "def make_elem_dict(osm_file):\n",
    "    \n",
    "    elem_dict = col.defaultdict(set)\n",
    "    \n",
    "    context = ET.iterparse(osm_file, events=('start', 'end'))\n",
    "    _, root = next(context)\n",
    "    \n",
    "    for event, elem in context:\n",
    "        if event == 'end' and (elem.tag == 'node' or elem.tag == 'way' or elem.tag == 'relation'):\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if tag.attrib['k'] == 'natural':\n",
    "                    for tag in elem.iter(\"tag\"):\n",
    "                        elem_dict[tag.attrib['k']].add(tag.attrib['v'])\n",
    "        root.clear()\n",
    "    return elem_dict\n",
    "\n",
    "pprint.pprint(make_elem_dict(OSM_FILE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some notes on checking the other fields:\n",
    "* 'wikipedia' - not going to touch, connects an elem to wikipedi article\n",
    "* wikidata - Connects to wikimedia commons. Googling 'wikidata and one of the values from the dict gives' a link to an [article](https://commons.wikimedia.org/wiki/File:Bayou_St_John_by_Spanish_Fort_2009.jpg) - not touching this\n",
    "* Scanning through others doesn't seem to throw any flags (trying to keep scope of investigation more targeted) However:\n",
    "* will fix capitalization in 'water'\n",
    "* in source - change any 'bing' to 'Bing', 'landsat' to 'LandSat'\n",
    "* Look at all 'note'. There are many non-'nature' elems with notes as well, so I'll have to write a script to pull all 'nature' elems that also have a 'note' tag elem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "OSM_FILE = \"/Users/mchana/GitHub/udacity/large_files/new-orleans_region.osm\"\n",
    "\n",
    "def count_elem(osm_file):\n",
    "    context = ET.iterparse(osm_file, events=('start', 'end'))\n",
    "    _, root = next(context)\n",
    "    \n",
    "    for event, elem in context:\n",
    "        if event == 'end' and (elem.tag == 'node' or elem.tag == 'way' or elem.tag == 'relation'):\n",
    "            for tag_elem in elem.iter(\"tag\"):\n",
    "                if tag_elem.attrib['k'] == 'natural':\n",
    "                    for tag_elem in elem.iter(\"tag\"):\n",
    "                        if tag_elem.attrib['k'] == 'note':\n",
    "                            print(ET.dump(elem))\n",
    "        root.clear()\n",
    "count_elem(OSM_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What I'm learing:\n",
    "* The comment: \"\"I have altered the natural:coatline tag as this way duplicates existing coastline ways\"\" tells me that I shouldn't mess with either of the 'coastline' or '_coastline' keys as it's purposful. Other notes don't indicate any other action needed.\n",
    "\n",
    "Back to dict:\n",
    "* 'name' - fix capitalization on lower case words There are enough that I can see they all need it. 'xxx' seems like an error but not going to change it...\n",
    "* 'fixme' - not touching. Seems like a way for people to know what needs to be changed due to construction, etc. (\"Needs survey\", \"tempoary way, whilst coastline is sorted out\", etc.) Still, slightly problematic as just states \"temporary fix\" for some. Not doing anything with this.\n",
    "* NHD:ReachCode - (\"Unique identifier composed of two parts, first eight digits = subbasin code as defined by FIPS 103, and next six digits = random-assigned sequential number unique within a Cataloguing Unit.\"\n",
    "    * issues that I see but can't fix:\n",
    "           * Is supposed to be a unique identifier but some have more than one. Manually inspecting one such element doesn't reveal why, but noticed also has a duplicate NHD:ComID. (WayID: 43393226)\n",
    "\n",
    "* NHD:FDate has one or more instances with two dates. Nothing I can do though.\n",
    "* NHD:FCode , ok to have more than one.\n",
    "* NDH:ComID, has more than one id for some, but more imporantly PDF indicates \"ComID field deleted from all feature classes/tables\" in the Model Changes section. Should this data still be here? Maybe contact NHD to check."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write scripts to perform cleaning:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decipher data.py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import codecs\n",
    "import pprint\n",
    "import re\n",
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "import inspect as ins\n",
    "\n",
    "import cerberus\n",
    "\n",
    "from supporting_files import test_schema\n",
    "\n",
    "# OSM_PATH = \"/Users/mchana/GitHub/udacity/large_files/new-orleans_region.osm\"\n",
    "# OSM_PATH= \"supporting_files/new-orleans_region_sample_k1000.osm\"\n",
    "# OSM_PATH= \"supporting_files/new-orleans_region_sample_k100.osm\"\n",
    "OSM_PATH= \"supporting_files/new-orleans_region_sample_k10.osm\"\n",
    "\n",
    "NODES_PATH = \"supporting_files/exports/nodes.csv\"\n",
    "NODES_TAGS_PATH = \"supporting_files/exports/nodes_tags.csv\"\n",
    "WAYS_PATH = \"supporting_files/exports/ways.csv\"\n",
    "WAYS_NODES_PATH = \"supporting_files/exports/ways_nodes.csv\"\n",
    "WAYS_TAGS_PATH = \"supporting_files/exports/ways_tags.csv\"\n",
    "RELATIONS_PATH = \"supporting_files/exports/relations.csv\"\n",
    "RELATIONS_MEMBERS_PATH = \"supporting_files/exports/relations_members.csv\"\n",
    "RELATIONS_TAGS_PATH = \"supporting_files/exports/relations_tags.csv\"\n",
    "\n",
    "\n",
    "LOWER_COLON = re.compile(r'^([a-z]|_)+:([a-z]|_)+')\n",
    "PROBLEMCHARS = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "SCHEMA = test_schema.schema\n",
    "\n",
    "# Make sure the fields order in the csvs matches the column order in the sql table schema\n",
    "NODES_FIELDS = ['id', 'lat', 'lon', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "NODES_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAYS_FIELDS = ['id', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "WAYS_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAYS_NODES_FIELDS = ['id', 'node_id', 'position']\n",
    "RELATIONS_FIELDS = ['id', 'user', 'uid', 'version', 'timestamp', 'changeset']\n",
    "RELATIONS_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "RELATIONS_MEMBERS_FIELDS = ['id', 'mem_id','type', 'role', 'position']\n",
    "\n",
    "\n",
    "# why node_attr_field, way_attr_field input params here?\n",
    "def shape_element(element, node_attr_fields=NODES_FIELDS, way_attr_fields=WAYS_FIELDS,\n",
    "                  problem_chars=PROBLEMCHARS, default_tag_type='regular'):\n",
    "    \"\"\"Clean and shape node or way XML element to Python dict\"\"\"\n",
    "\n",
    "    # holds dicts of 'tag' elements\n",
    "    tags = []\n",
    "    \n",
    "    # creates dicts for 'tag' elements\n",
    "    for child in element:\n",
    "            if child.tag == 'tag':\n",
    "                # end loop execution if problematic characters found\n",
    "                if PROBLEMCHARS.search(child.attrib['k']):\n",
    "                    pass\n",
    "                # creates dict to be added to tags list\n",
    "                else:\n",
    "                    tag_dict = {'id':element.attrib['id'],\n",
    "                                'key':child.attrib['k'],\n",
    "                                'value':child.attrib['v'],\n",
    "                                'type':default_tag_type \n",
    "                               }\n",
    "\n",
    "                    # check for colon in string, fix dict accordingly\n",
    "                    if LOWER_COLON.search(tag_dict['key']):\n",
    "                        key_split = tag_dict['key'].split(':',1)\n",
    "                        tag_dict['key'] = key_split[1]\n",
    "                        tag_dict['type'] = key_split[0]\n",
    "\n",
    "                    # append tags list with newly created dict\n",
    "                    tags.append(tag_dict)\n",
    "            \n",
    "    if element.tag == 'node':\n",
    "        node_attribs = {'id':int(element.attrib['id']),\n",
    "                   'user':element.attrib['user'],\n",
    "                   'uid':int(element.attrib['uid']),\n",
    "                   'version':element.attrib['version'],\n",
    "                   'lat':float(element.attrib['lat']),\n",
    "                    'lon':float(element.attrib['lon']),\n",
    "                    'timestamp':element.attrib['timestamp'],\n",
    "                    'changeset':int(element.attrib['changeset'])\n",
    "                   }\n",
    "        return {'node': node_attribs, 'node_tags': tags}\n",
    "    \n",
    "    elif element.tag == 'way':\n",
    "        way_attribs = {'id':int(element.attrib['id']),\n",
    "                      'user':element.attrib['user'],\n",
    "                      'uid':int(element.attrib['uid']),\n",
    "                      'version':element.attrib['version'],\n",
    "                      'timestamp':element.attrib['timestamp'],\n",
    "                      'changeset':int(element.attrib['changeset'])\n",
    "                      }\n",
    "        \n",
    "        # holds list of dicts for 'nd' elements\n",
    "        way_nodes = []\n",
    "        \n",
    "        # counter to increment instances of 'nd' tags\n",
    "        nd_counter = 0\n",
    "        \n",
    "        for child in element:\n",
    "            if child.tag == 'nd':\n",
    "                nd_dict = {'id':element.attrib['id'],\n",
    "                          'node_id':int(child.attrib['ref']),\n",
    "                          'position':nd_counter} \n",
    "                nd_counter += 1\n",
    "                way_nodes.append(nd_dict)\n",
    "        \n",
    "        return {'way': way_attribs, 'way_nodes': way_nodes, 'way_tags': tags}\n",
    "    \n",
    "    elif element.tag == 'relation':\n",
    "        rel_attribs = {\n",
    "            'id':int(element.attrib['id']),\n",
    "            'user':element.attrib['user'],\n",
    "            'uid':int(element.attrib['version']),\n",
    "            'version':element.attrib['version'],\n",
    "            'timestamp':element.attrib['timestamp'],\n",
    "            'changeset':int(element.attrib['changeset'])\n",
    "        }\n",
    "        \n",
    "        rel_members = []\n",
    "        \n",
    "        mem_counter = 0\n",
    "        \n",
    "        for child in element:\n",
    "            if child.tag == 'member':\n",
    "                mem_dict = {\n",
    "                    'id':element.attrib['id'],\n",
    "                    'mem_id':int(child.attrib['ref']),\n",
    "                    'type':child.attrib['type'],\n",
    "                    'role':child.attrib['role'],\n",
    "                    'position':mem_counter\n",
    "                }\n",
    "                mem_counter += 1\n",
    "                rel_members.append(mem_dict)\n",
    "        \n",
    "        return {'relation': rel_attribs, 'relation_members': rel_members, 'relation_tags': tags}\n",
    "    \n",
    "\n",
    "\n",
    "# ================================================== #\n",
    "#               Helper Functions                     #\n",
    "# ================================================== #\n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    \n",
    "    context = ET.iterparse(osm_file, events=('start', 'end'))\n",
    "    _, root = next(context)\n",
    "    \n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            for tag_elem in elem.iter(\"tag\"):\n",
    "                if tag_elem.attrib['k'] == 'natural':\n",
    "                    yield elem\n",
    "        root.clear()\n",
    "\n",
    "# takes in ET.element obj, validator object, schema\n",
    "def validate_element(element, validator, schema=SCHEMA):\n",
    "    \"\"\"Raise ValidationError if element does not match schema\"\"\"\n",
    "    if validator.validate(element, schema) is not True:\n",
    "        field, errors = next(validator.errors.iteritems())\n",
    "        message_string = \"\\nElement of type '{0}' has the following errors:\\n{1}\"\n",
    "        error_string = pprint.pformat(errors)\n",
    "        \n",
    "        raise Exception(message_string.format(field, error_string))\n",
    "\n",
    "# ================================================== #\n",
    "#               Main Function                        #\n",
    "# ================================================== #\n",
    "\n",
    "# file_in=OSM file, validate=True or False\n",
    "def process_map(file_in, validate):\n",
    "    \n",
    "    # with-open files in write mode\n",
    "    with codecs.open(NODES_PATH, 'w') as nodes_file, \\\n",
    "        codecs.open(NODES_TAGS_PATH, 'w') as nodes_tags_file, \\\n",
    "        codecs.open(WAYS_PATH, 'w') as ways_file, \\\n",
    "        codecs.open(WAYS_NODES_PATH, 'w') as ways_nodes_file, \\\n",
    "        codecs.open(WAYS_TAGS_PATH, 'w') as ways_tags_file, \\\n",
    "        codecs.open(RELATIONS_PATH, 'w') as relations_file, \\\n",
    "        codecs.open(RELATIONS_MEMBERS_PATH, 'w') as relations_members_file, \\\n",
    "        codecs.open(RELATIONS_TAGS_PATH, 'w') as relations_tags_file:\n",
    "        \n",
    "        # create writer objects\n",
    "        nodes_writer = csv.DictWriter(nodes_file, NODES_FIELDS)\n",
    "        nodes_tags_writer = csv.DictWriter(nodes_tags_file, NODES_TAGS_FIELDS)\n",
    "        ways_writer = csv.DictWriter(ways_file, WAYS_FIELDS)\n",
    "        ways_nodes_writer = csv.DictWriter(ways_nodes_file, WAYS_NODES_FIELDS)\n",
    "        ways_tags_writer = csv.DictWriter(ways_tags_file, WAYS_TAGS_FIELDS)\n",
    "        relations_writer = csv.DictWriter(relations_file, RELATIONS_FIELDS)\n",
    "        relations_members_writer = csv.DictWriter(relations_members_file, RELATIONS_MEMBERS_FIELDS)\n",
    "        relations_tags_writer = csv.DictWriter(relations_tags_file, RELATIONS_TAGS_FIELDS)\n",
    "        \n",
    "        # write headers using field names specified in DictWriter constructor\n",
    "        nodes_writer.writeheader()\n",
    "        nodes_tags_writer.writeheader()\n",
    "        ways_writer.writeheader()\n",
    "        ways_nodes_writer.writeheader()\n",
    "        ways_tags_writer.writeheader()\n",
    "        relations_writer.writeheader()\n",
    "        relations_members_writer.writeheader()\n",
    "        relations_tags_writer.writeheader()\n",
    "\n",
    "        # the Validator class object instantiated here is callable to normalize \n",
    "        # and/or validate any mapping against validation schema \n",
    "        validator = cerberus.Validator()\n",
    "\n",
    "        # loop over generator obj from get_element()\n",
    "            # get_element() takes OSM file and tags I'm interested in\n",
    "        for element in get_element(file_in, tags=('node', 'way', 'relation')):\n",
    "            # create a shape_element() object\n",
    "                # takes in the element from iterator, outputs a dict\n",
    "            el = shape_element(element)\n",
    "            pprint.pprint(el)\n",
    "            \n",
    "            # if dict from 'el' exists, \n",
    "            if el:\n",
    "                if validate is True:\n",
    "                    # calls validate_element() which raises error if dict doesn't match schema\n",
    "                        # takes in dict from el, and validator obj\n",
    "                    validate_element(el, validator)\n",
    "                # write each dict to appropriate writer obj\n",
    "                if element.tag == 'node':\n",
    "                    nodes_writer.writerow(el['node'])\n",
    "                    nodes_tags_writer.writerows(el['node_tags'])\n",
    "                elif element.tag == 'way':\n",
    "                    ways_writer.writerow(el['way'])\n",
    "                    ways_nodes_writer.writerows(el['way_nodes'])\n",
    "                    ways_tags_writer.writerows(el['way_tags'])\n",
    "                elif element.tag == 'relation':\n",
    "                    relations_writer.writerow(el['relation'])\n",
    "                    relations_members_writer.writerows(el['relation_members'])\n",
    "                    relations_tags_writer.writerows(el['relation_tags'])\n",
    "                    \n",
    "if __name__ == '__main__':\n",
    "    process_map(OSM_PATH, validate=False)\n",
    "\n",
    "# process_map called\n",
    "    # with-open files to be written\n",
    "    # create writer objects\n",
    "    # create class validator object\n",
    "    # iterate over each element from a generator created by get_element()\n",
    "        # create a dict with the element\n",
    "        # validate dict against a schema using validate_element()\n",
    "        # write it to csv using appropriate writer object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "    # √update shape_element() to suit my needs\n",
    "    # √update process_map() to write relation CSVs as well\n",
    "        # verify validity of data I've written (logic of code)\n",
    "            # possible uid and user problem in relation dicts # create fixer functions for each field I want to fix as a separate script\n",
    "            # invoke each helper function where added to dict in shape_element() function. Ex: fixer_function(child.attrib['v'])\n",
    "    # prepare schema and schema validation (validate=False temporarily. Change after fixing schema)\n",
    "        # descipher schema file and validation functionality\n",
    "            # validator class object is created\n",
    "            # dict and validator class object fed into validate_element()\n",
    "                # validates and throws error (stops execution) if validation not True\n",
    "                # else, execution continues and all items written to the csv\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "OSM_FILE = \"/Users/mchana/GitHub/udacity/large_files/new-orleans_region.osm\"\n",
    "SAMPLE_FILE = \"supporting_files/new-orleans_region_sample_k10.osm\"\n",
    "\n",
    "k = 10 # Parameter: take every k-th top level element\n",
    "\n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    \"\"\"Yield element if it is the right type of tag\n",
    "\n",
    "    Reference:\n",
    "    http://stackoverflow.com/questions/3095434/inserting-newlines-in-xml-file-generated-via-xml-etree-elementtree-in-python\n",
    "    \"\"\"\n",
    "    context = ET.iterparse(osm_file, events=('start', 'end'))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "        root.clear()\n",
    "\n",
    "\n",
    "with open(SAMPLE_FILE, 'w') as output:\n",
    "    output.write('<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n')\n",
    "    output.write('<osm>\\n  ')\n",
    "\n",
    "    # Write every kth top level element\n",
    "    for i, element in enumerate(get_element(OSM_FILE)):\n",
    "        if i % k == 0:\n",
    "            output.write(ET.tostring(element, encoding='unicode'))\n",
    "\n",
    "    output.write('</osm>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:bunnies]",
   "language": "python",
   "name": "conda-env-bunnies-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
