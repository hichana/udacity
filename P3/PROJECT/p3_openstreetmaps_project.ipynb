{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse and clean OpenStreetMap data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify, archive, document data file:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data File: New Orleans [MapZen Extract](https://mapzen.com/data/metro-extracts/metro/new-orleans_louisiana/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect and document general structure:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essential task is to parse data file and clean. What am I parsing?\n",
    "\n",
    "First, looking at the documentation, the gist that I get is:\n",
    "* node is a lon/lat point, possibly with 'tag' child elements\n",
    "* way has nodes as children(as nd elements) and defines things like roads, buildings, etc. Also may have 'tag' children\n",
    "* relation has nested 'member' elements which refer to ways and nodes - defines relationships among elements\n",
    "\n",
    "Next, open doc in vim - get aquanted, why? Because I can and I want to. Visually I see (screenshots):\n",
    "* standard opening:  <?xml version='1.0' encoding='UTF-8'?>\n",
    "* root is an 'osm' tag\n",
    "* doc is 16,082,009 lines long in total - wow!\n",
    "* skipping down some pages, looks like a bunch of nodes with no internal element defined at top\n",
    "* skipping down from middle of doc, looks like a bunch of 'way' tags of natural features (streams, other waterway)\n",
    "* skipping up from the end of the doc, looks like a bunch of 'relations' of things like gardens and public transportation stations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speculate on what to work with specifically:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what should I clean here? 'Tag' elements seem to be an important part where details about map features are stored. This should also be where a lot of user-generated/added content is stored.\n",
    "\n",
    "In the middle, what is NHD? Strikes me as interesting. So Googling it, Ah, US Geological Survey's National Hydrography Dataset (Watershed Boundary Dataset). The data should be damn near perfect. Simply, it should be a way, with node child element references and tags with all relevant info (so  poly boundary and some meta data). These are mostly 'natural' features - auditing them sounds interesting.\n",
    "\n",
    "First, the NHD data shouldn't really have many errors given the data is taken from the NHD and uploaded to OpenStreetMaps (as opposed to multiple user-generated content for most contributor use-cases of OpenStreetMap). It's worth checking a few things to be sure. Some inspection is in order:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect/audit/plan out cleaning methodology:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, what keys exist in natural feature tag elements as a whole?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "OSM_FILE = \"/Users/mchana/GitHub/udacity/large_files/new-orleans_region.osm\"\n",
    "\n",
    "def count_elem(osm_file):\n",
    "    tag_set = set() \n",
    "    \n",
    "    context = ET.iterparse(osm_file, events=('start', 'end'))\n",
    "    _, root = next(context)\n",
    "    \n",
    "    for event, elem in context:\n",
    "        if event == 'end' and (elem.tag == 'node' or elem.tag == 'way' or elem.tag == 'relation'):\n",
    "            for tag_elem in elem.iter(\"tag\"):\n",
    "                if tag_elem.attrib['k'] == 'natural':\n",
    "                    for tag_elem in elem.iter(\"tag\"):\n",
    "                        tag_set.add(tag_elem.attrib['k'])\n",
    "        root.clear()\n",
    "    return tag_set\n",
    "\n",
    "keys_list = list(count_elem(OSM_FILE))\n",
    "print(keys_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hard to tell visually - sort it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted(keys_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the k values like 'golf' seem pretty arbitrary, but they may have a purpose. Others, like NHD:FCode seem redundant with FCode. Maybe US Geological society changed their naming convention at one point. There may be something to clean here... I also see a 'fixme' key which may be something. Worth noting for now, but what will really help is to create a dictionary of the features and their attribute contents.\n",
    "\n",
    "Some notes:\n",
    "* FCODE is used at just once - uses '46600' as its contents. Jumping down to NHD:FCode I see more contents and again, the '46600'. I'm betting FCODE and NHD:FCode are the same. Looking at the NHD poster I find that 46600 is for \"Swamp/Marsh\". The different keys here are definitely trying to achieve the same thing. They can be merged. Also, I'm not worried about a key having multiple FCode - for example 33600;46600 is a Canal/Ditch, but it's also a Swamp/Marsh. \n",
    "* Next redundant tag key is FDATE. 'FDATE' has 'Mon May 23 00:00:00 CEST 2011'as its contents. Looks like a time stamp - checking. I'll check to make sure other elements with the FDATE tag elem have a different timestamp. I could do this, but there's a better - faster way (all I need to do is see a few others to verify it's a time stamp. Vim - /search_key for 'FDATE\". Sure enough, found on line 13,356,160. Vim - n finds no other instances! What about NHD:Fdate? Yes, all kinds of others, but their dates ar emore simple - for example, \"2005/12/05\". Fair enough to say that the rogue element's date can be changed to 2011/05/23. \n",
    "* Now thinking, there only seems to be a single rogue elem. Inspect it to make sure it's not wildly different. Nothing stands out - /search for the user Aleks-Berlin. He also seems to have edited a few others - some tags called \"FIXME\", which seem a little haphazard. I'm wondering who this person is and if entering data haphazardly. Check element on map: /search for one of the nodes, then enter lat/lon coordinates on map: 29.3557244, -90.0538116. Looks like a valid natural feature, so the way elem should stay in place. \n",
    "* Next, FTYPE. 466 corresponds to Feature Type from pdf. \n",
    "* 'natural' all good\n",
    "* 'PERMANENT_ is redundant. Has strange value: &#123;A0AFF249-A7D2-44F4-AD8A-0A4A68F99450&#125;\n",
    "    * Search for NHD:Permanent_. They all have much simpler values like \"151098380\". What is it? On PDF there is a 'Permanent_Identifier' feature that is a 40 character string, but all the ones I find are 9 characters. Not enough info here - so leaving this one alone. Somehow can flag this elem? Not sure what this is so leaving alone.\n",
    "*RESOLUTION is '2'. From NHD documentation, this should be set to \"High\" (Code of source resolution: 1=Local resolution, 2=High resolution, 3=Medium Resolution.)\n",
    "* SHAPE_AREA and SHAPE_LENG exist in PDF guide as 'Shape_Area' and 'Shape_Length' but have no guidelines and are not used in any other elements in the XML file. Again, wondering if best to just remove this element!\n",
    "\n",
    "    \n",
    "Now onto auditing some some of the other keys. For this it's best to build the dictionary and see what kind of information they hold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import collections as col\n",
    "import pprint\n",
    "\n",
    "OSM_FILE = \"/Users/mchana/GitHub/udacity/large_files/new-orleans_region.osm\"\n",
    "\n",
    "def make_elem_dict(osm_file):\n",
    "    \n",
    "    elem_dict = col.defaultdict(set)\n",
    "    \n",
    "    context = ET.iterparse(osm_file, events=('start', 'end'))\n",
    "    _, root = next(context)\n",
    "    \n",
    "    # opportunity for 'continue' here...\n",
    "    # also, pull out - make function to find correct element\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and (elem.tag == 'node' or elem.tag == 'way' or elem.tag == 'relation'):\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if tag.attrib['k'] == 'natural':\n",
    "                    # am overwriting tag here from prev. loop - call something else\n",
    "                    for tag in elem.iter(\"tag\"):\n",
    "                        elem_dict[tag.attrib['k']].add(tag.attrib['v'])\n",
    "        root.clear()\n",
    "    return elem_dict\n",
    "\n",
    "pprint.pprint(make_elem_dict(OSM_FILE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some notes on checking the other fields starting at the end:\n",
    "* 'wikipedia' - not going to touch, connects an elem to wikipedi article\n",
    "* wikidata - Connects to wikimedia commons. Googling 'wikidata and one of the values from the dict gives' a link to an [article](https://commons.wikimedia.org/wiki/File:Bayou_St_John_by_Spanish_Fort_2009.jpg) - not touching this\n",
    "* Scanning through others doesn't seem to throw any flags (trying to keep scope of investigation more targeted) However:\n",
    "* will fix capitalization in 'water' tag\n",
    "* in source - change any 'bing' to 'Bing', 'landsat' to 'LandSat'\n",
    "* Look at all 'note'. There are many non-'nature' elems with notes as well, so I'll have to write a script to pull all 'nature' elems that also have a 'note' tag elem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "OSM_FILE = \"/Users/mchana/GitHub/udacity/large_files/new-orleans_region.osm\"\n",
    "\n",
    "def find_note_tag(osm_file):\n",
    "    context = ET.iterparse(osm_file, events=('start', 'end'))\n",
    "    _, root = next(context)\n",
    "    \n",
    "    for event, elem in context:\n",
    "        if event == 'end' and (elem.tag == 'node' or elem.tag == 'way' or elem.tag == 'relation'):\n",
    "            for tag_elem in elem.iter(\"tag\"):\n",
    "                if tag_elem.attrib['k'] == 'natural':\n",
    "                    for tag_elem in elem.iter(\"tag\"):\n",
    "                        if tag_elem.attrib['k'] == 'note':\n",
    "                            print(ET.dump(elem))\n",
    "        root.clear()\n",
    "find_note_tag(OSM_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What I'm learing: (add PDF descriptions for each field when introducing them)\n",
    "* The comment: \"\"I have altered the natural:coatline tag as this way duplicates existing coastline ways\"\" tells me that I shouldn't mess with either of the 'coastline' or '_coastline' keys as it's purposful. Other notes don't indicate any other action needed.\n",
    "\n",
    "Back to dict:\n",
    "* 'name' - fix capitalization on lower case words There are enough that I can see they all need it. 'xxx' seems like an error but not going to change it...\n",
    "    * discard 'yes' in node 4506654389\n",
    "* 'fixme' - not touching. Seems like a way for people to know what needs to be changed due to construction, etc. (\"Needs survey\", \"tempoary way, whilst coastline is sorted out\", etc.) Still, slightly problematic as just states \"temporary fix\" for some. Not doing anything with this.\n",
    "* NHD:ReachCode - (\"Unique identifier composed of two parts, first eight digits = subbasin code as defined by FIPS 103, and next six digits = random-assigned sequential number unique within a Cataloguing Unit.\"\n",
    "    * issues that I see but can't fix:\n",
    "           * Is supposed to be a unique identifier but some have more than one. Manually inspecting one such element doesn't reveal why, but noticed also has a duplicate NHD:ComID. (WayID: 43393226)\n",
    "\n",
    "* NHD:FDate has one or more instances with two dates. Nothing I can do though.\n",
    "* NHD:FCode , ok to have more than one.\n",
    "    * NDH:ComID, has more than one id for some, but more imporantly PDF indicates \"ComID field deleted from all feature classes/tables\" in the Model Changes section. Should this data still be here? This is the most updated model documentation from August 2016. Waiting on email sent to NHD..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build basic parser/CSV compiler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import codecs\n",
    "import pprint\n",
    "import re\n",
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "import inspect as ins\n",
    "\n",
    "import cerberus\n",
    "\n",
    "from supporting_files import test_schema\n",
    "from supporting_files import fix_dict as fd\n",
    "\n",
    "OSM_PATH = \"/Users/mchana/GitHub/udacity/large_files/new-orleans_region.osm\"\n",
    "# OSM_PATH= \"supporting_files/exports/new-orleans_region_sample_k1000.osm\"\n",
    "# OSM_PATH= \"supporting_files/exports/new-orleans_region_sample_k100.osm\"\n",
    "# OSM_PATH= \"supporting_files/exports/new-orleans_region_sample_k10.osm\"\n",
    "\n",
    "NODES_PATH = \"supporting_files/exports/nodes.csv\"\n",
    "NODES_TAGS_PATH = \"supporting_files/exports/nodes_tags.csv\"\n",
    "WAYS_PATH = \"supporting_files/exports/ways.csv\"\n",
    "WAYS_NODES_PATH = \"supporting_files/exports/ways_nodes.csv\"\n",
    "WAYS_TAGS_PATH = \"supporting_files/exports/ways_tags.csv\"\n",
    "RELATIONS_PATH = \"supporting_files/exports/relations.csv\"\n",
    "RELATIONS_MEMBERS_PATH = \"supporting_files/exports/relations_members.csv\"\n",
    "RELATIONS_TAGS_PATH = \"supporting_files/exports/relations_tags.csv\"\n",
    "\n",
    "\n",
    "LOWER_UPPER_COLON = re.compile(r'^([a-z]|_)+:([a-z]|_)+', re.IGNORECASE)\n",
    "PROBLEMCHARS = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "SCHEMA = test_schema.project_schema\n",
    "\n",
    "# Make sure the fields order in the csvs matches the column order in the sql table schema\n",
    "NODES_FIELDS = ['id', 'lat', 'lon', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "NODES_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAYS_FIELDS = ['id', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "WAYS_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAYS_NODES_FIELDS = ['id', 'node_id', 'position']\n",
    "RELATIONS_FIELDS = ['id', 'user', 'uid', 'version', 'timestamp', 'changeset']\n",
    "RELATIONS_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "RELATIONS_MEMBERS_FIELDS = ['id', 'mem_id','type', 'role', 'position']\n",
    "\n",
    "\n",
    "# why node_attr_field, way_attr_field input params here?\n",
    "def shape_element(element, node_attr_fields=NODES_FIELDS, way_attr_fields=WAYS_FIELDS,\n",
    "                  problem_chars=PROBLEMCHARS, default_tag_type='regular'):\n",
    "    \"\"\"Clean and shape node or way XML element to Python dict\"\"\"\n",
    "\n",
    "    # holds dicts of 'tag' elements\n",
    "    tags = []\n",
    "    \n",
    "    # creates dicts for 'tag' elements\n",
    "    for child in element:\n",
    "            if child.tag != 'tag' or PROBLEMCHARS.search(child.attrib['k']):\n",
    "                continue\n",
    "            tag_dict = {'id':element.attrib['id'],\n",
    "                        'key':child.attrib['k'],\n",
    "                        'value':child.attrib['v'],\n",
    "                        'type':default_tag_type \n",
    "                       }\n",
    "\n",
    "            if LOWER_UPPER_COLON.search(tag_dict['key']):\n",
    "                key_split = tag_dict['key'].split(':',1)\n",
    "                tag_dict['key'] = key_split[1]\n",
    "                tag_dict['type'] = key_split[0]\n",
    "\n",
    "            tags.append(tag_dict)\n",
    "\n",
    "    if element.tag == 'node':\n",
    "        node_attribs = {'id':element.attrib['id'],\n",
    "                   'user':element.attrib['user'],\n",
    "                   'uid':element.attrib['uid'],\n",
    "                   'version':element.attrib['version'],\n",
    "                   'lat':element.attrib['lat'],\n",
    "                    'lon':element.attrib['lon'],\n",
    "                    'timestamp':element.attrib['timestamp'],\n",
    "                    'changeset':element.attrib['changeset']\n",
    "                   }\n",
    "        return {'node': node_attribs, 'node_tags': tags}\n",
    "    \n",
    "    elif element.tag == 'way':\n",
    "        way_attribs = {'id':element.attrib['id'],\n",
    "                      'user':element.attrib['user'],\n",
    "                      'uid':element.attrib['uid'],\n",
    "                      'version':element.attrib['version'],\n",
    "                      'timestamp':element.attrib['timestamp'],\n",
    "                      'changeset':element.attrib['changeset']\n",
    "                      }\n",
    "        \n",
    "        # holds list of dicts for 'nd' elements\n",
    "        way_nodes = []\n",
    "        \n",
    "        # counter to increment instances of 'nd' tags\n",
    "        nd_counter = 0\n",
    "        \n",
    "        for child in element:\n",
    "            if child.tag == 'nd':\n",
    "                nd_dict = {'id':element.attrib['id'],\n",
    "                          'node_id':child.attrib['ref'],\n",
    "                          'position':nd_counter} \n",
    "                nd_counter += 1\n",
    "                way_nodes.append(nd_dict)\n",
    "        \n",
    "        return {'way': way_attribs, 'way_nodes': way_nodes, 'way_tags': tags}\n",
    "    \n",
    "    elif element.tag == 'relation':\n",
    "        rel_attribs = {\n",
    "            'id':element.attrib['id'],\n",
    "            'user':element.attrib['user'],\n",
    "            'uid':element.attrib['uid'],\n",
    "            'version':element.attrib['version'],\n",
    "            'timestamp':element.attrib['timestamp'],\n",
    "            'changeset':element.attrib['changeset']\n",
    "        }\n",
    "        \n",
    "        rel_members = []\n",
    "        \n",
    "        mem_counter = 0\n",
    "        \n",
    "        for child in element:\n",
    "            if child.tag == 'member':\n",
    "                mem_dict = {\n",
    "                    'id':element.attrib['id'],\n",
    "                    'mem_id':child.attrib['ref'],\n",
    "                    'type':child.attrib['type'],\n",
    "                    'role':child.attrib['role'],\n",
    "                    'position':mem_counter\n",
    "                }\n",
    "                mem_counter += 1\n",
    "                rel_members.append(mem_dict)\n",
    "        \n",
    "        return {'relation': rel_attribs, 'relation_members': rel_members, 'relation_tags': tags}\n",
    "    \n",
    "# ================================================== #\n",
    "#               Helper Functions                     #\n",
    "# ================================================== #\n",
    "def get_elements(osm_file, tags=('node', 'way', 'relation')):\n",
    "    \n",
    "    context = ET.iterparse(osm_file, events=('start', 'end'))\n",
    "    _, root = next(context)\n",
    "    \n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            for tag_elem in elem.iter(\"tag\"):\n",
    "                if tag_elem.attrib['k'] == 'natural':\n",
    "                    yield elem\n",
    "        root.clear()\n",
    "\n",
    "# takes in ET.element obj, validator object, schema\n",
    "def validate_element(element, validator, schema=SCHEMA):\n",
    "    \"\"\"Raise ValidationError if element does not match schema\"\"\"\n",
    "    if validator.validate(element, schema) is not True:\n",
    "        field, errors = next(validator.errors.items())\n",
    "        message_string = \"\\nElement of type '{0}' has the following errors:\\n{1}\"\n",
    "        error_string = pprint.pformat(errors)\n",
    "        \n",
    "        raise Exception(message_string.format(field, error_string))\n",
    "\n",
    "# ================================================== #\n",
    "#               Main Function                        #\n",
    "# ================================================== #\n",
    "\n",
    "# file_in=OSM file, validate=True or False\n",
    "def process_map(file_in, validate):\n",
    "    \n",
    "    # with-open files in write mode\n",
    "    with codecs.open(NODES_PATH, 'w') as nodes_file, \\\n",
    "        codecs.open(NODES_TAGS_PATH, 'w') as nodes_tags_file, \\\n",
    "        codecs.open(WAYS_PATH, 'w') as ways_file, \\\n",
    "        codecs.open(WAYS_NODES_PATH, 'w') as ways_nodes_file, \\\n",
    "        codecs.open(WAYS_TAGS_PATH, 'w') as ways_tags_file, \\\n",
    "        codecs.open(RELATIONS_PATH, 'w') as relations_file, \\\n",
    "        codecs.open(RELATIONS_MEMBERS_PATH, 'w') as relations_members_file, \\\n",
    "        codecs.open(RELATIONS_TAGS_PATH, 'w') as relations_tags_file:\n",
    "        \n",
    "        # create writer objects\n",
    "        nodes_writer = csv.DictWriter(nodes_file, NODES_FIELDS)\n",
    "        nodes_tags_writer = csv.DictWriter(nodes_tags_file, NODES_TAGS_FIELDS)\n",
    "        ways_writer = csv.DictWriter(ways_file, WAYS_FIELDS)\n",
    "        ways_nodes_writer = csv.DictWriter(ways_nodes_file, WAYS_NODES_FIELDS)\n",
    "        ways_tags_writer = csv.DictWriter(ways_tags_file, WAYS_TAGS_FIELDS)\n",
    "        relations_writer = csv.DictWriter(relations_file, RELATIONS_FIELDS)\n",
    "        relations_members_writer = csv.DictWriter(relations_members_file, RELATIONS_MEMBERS_FIELDS)\n",
    "        relations_tags_writer = csv.DictWriter(relations_tags_file, RELATIONS_TAGS_FIELDS)\n",
    "        \n",
    "        # write headers using field names specified in DictWriter constructor\n",
    "        nodes_writer.writeheader()\n",
    "        nodes_tags_writer.writeheader()\n",
    "        ways_writer.writeheader()\n",
    "        ways_nodes_writer.writeheader()\n",
    "        ways_tags_writer.writeheader()\n",
    "        relations_writer.writeheader()\n",
    "        relations_members_writer.writeheader()\n",
    "        relations_tags_writer.writeheader()\n",
    "\n",
    "        # the Validator class object instantiated here is callable to normalize \n",
    "        # and/or validate any mapping against validation schema \n",
    "        validator = cerberus.Validator()\n",
    "\n",
    "        # loop over generator obj from get_element()\n",
    "            # get_element() takes OSM file and tags I'm interested in\n",
    "        for element in get_elements(file_in, tags=('node', 'way', 'relation')):\n",
    "            # create a shape_element() object\n",
    "                # takes in the element from iterator, outputs a dict\n",
    "            el = shape_element(element)\n",
    "#             pprint.pprint(el)\n",
    "\n",
    "            # cleans data in dict\n",
    "            el2 = fd.fix_dict(el)\n",
    "                   \n",
    "            if not el:\n",
    "                continue\n",
    "            if validate is True:\n",
    "                validate_element(el2, validator)\n",
    "            # write each dict to appropriate writer obj\n",
    "            if element.tag == 'node':\n",
    "                nodes_writer.writerow(el2['node'])\n",
    "                nodes_tags_writer.writerows(el2['node_tags'])\n",
    "            elif element.tag == 'way':\n",
    "                ways_writer.writerow(el2['way'])\n",
    "                ways_nodes_writer.writerows(el2['way_nodes'])\n",
    "                ways_tags_writer.writerows(el2['way_tags'])\n",
    "            elif element.tag == 'relation':\n",
    "                relations_writer.writerow(el2['relation'])\n",
    "                relations_members_writer.writerows(el2['relation_members'])\n",
    "                relations_tags_writer.writerows(el2['relation_tags'])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    process_map(OSM_PATH, validate=True)\n",
    "\n",
    "# process_map called\n",
    "    # with-open files to be written\n",
    "    # create writer objects\n",
    "    # write headers\n",
    "    # create class validator object\n",
    "    # iterate over each element from a generator created by get_element()\n",
    "        # create a dict from the element\n",
    "        # FIXER FUNCTION HERE!!!\n",
    "        # validate dict against a schema using validate_element()\n",
    "        # write it to csv using appropriate writer object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confirm basic parser/CSV compiler working properly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# open each CSV and check manually - VIM search in large OSM and check\n",
    "    # √nodes_tag.csv\n",
    "    # √nodes.csv\n",
    "    # √relation_members.csv\n",
    "    # √relations_tags.csv\n",
    "    # √relations.csv\n",
    "    # √ways_nodes.csv\n",
    "    # √ways_tags.csv\n",
    "    # √ways.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write cleaning scripts - insert into data.py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#√ Fix wayID 321535489\n",
    "    # No need to fix - it's inefficient to parse every line to clean a single element.\n",
    "        # better approach to do manually with access to the actual database\n",
    "#√ All NHD:FTYPE should be NDF:FType to conform to NHD data model - same with RESOLUTION should be Resolution\n",
    "#√ Fix capitalization in tag elems with 'water' key\n",
    "#√ Change 'bing' to 'Bing' and 'landsat' to 'Landsat' in tag elems with 'source' key\n",
    "    # maybe regex compare based on capitalization - then fix to first letter capitalized?\n",
    "#√ Fix tags with 'name' elem (fapitalization of firsr letter of each word)\n",
    "#not doing: discard node tag elem 4506654389"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update schema - implement schema validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#√ descipher schema file and validation functionality\n",
    "    #√ validator class object is created\n",
    "    #√ dict and validator class object fed into validate_element()\n",
    "        # validates and throws error (stops execution) if validation not True\n",
    "        # else, execution continues and all items written to the csv\n",
    "#√ update schema doc and run/verify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform statistical analysis using database queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First import CSV files into database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import csv\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlite_file = 'osm_db.db'\n",
    "conn = sqlite3.connect(sqlite_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute('''DROP TABLE IF EXISTS nodes_tags''')\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the table, specifying the column names and data types:\n",
    "cur.execute('''\n",
    "CREATE TABLE nodes (\n",
    "    id INTEGER PRIMARY KEY NOT NULL,\n",
    "    lat REAL,\n",
    "    lon REAL,\n",
    "    user TEXT,\n",
    "    uid INTEGER,\n",
    "    version INTEGER,\n",
    "    changeset INTEGER,\n",
    "    timestamp TEXT\n",
    ")\n",
    "''')\n",
    "# commit the changes\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
