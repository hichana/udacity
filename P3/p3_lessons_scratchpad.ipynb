{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P3 Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* be weary of the accuracy of data\n",
    "* Some basic steps in assessing data\n",
    "    * Test assumptions about values (what values does the data have?), data types and shape of data\n",
    "    * Identify error or ourliers\n",
    "    * Find missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tabular data:\n",
    "* Row (items) is a whole row\n",
    "* Field is a column\n",
    "* Value is a column meeting a row (single cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing CSV Files Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Starting Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Your task is to read the input DATAFILE line by line, and for the first 10 lines (not including the header)\n",
    "# split each line on \",\" and then for each line, create a dictionary\n",
    "# where the key is the header title of the field, and the value is the value of that field in the row.\n",
    "# The function parse_file should return a list of dictionaries,\n",
    "# each data line in the file being a single list entry.\n",
    "# Field names and values should not contain extra whitespace, like spaces or newline characters.\n",
    "# You can use the Python string method strip() to remove the extra whitespace.\n",
    "# You have to parse only the first 10 data lines in this exercise,\n",
    "# so the returned list should have 10 entries!\n",
    "\n",
    "import os\n",
    "\n",
    "DATADIR = \"\"\n",
    "DATAFILE = \"beatles-diskography.csv\"\n",
    "\n",
    "def parse_file(datafile):\n",
    "    data = []\n",
    "    with open(datafile, \"r\") as f:\n",
    "        for line in f:\n",
    "            print(line)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def test():\n",
    "    # a simple test of your implemetation\n",
    "    datafile = os.path.join(DATADIR, DATAFILE)\n",
    "    d = parse_file(datafile)\n",
    "    firstline = {'Title': 'Please Please Me', 'UK Chart Position': '1', 'Label': 'Parlophone(UK)', 'Released': '22 March 1963', 'US Chart Position': '-', 'RIAA Certification': 'Platinum', 'BPI Certification': 'Gold'}\n",
    "    tenthline = {'Title': '', 'UK Chart Position': '1', 'Label': 'Parlophone(UK)', 'Released': '10 July 1964', 'US Chart Position': '-', 'RIAA Certification': '', 'BPI Certification': 'Gold'}\n",
    "\n",
    "    assert d[0] == firstline\n",
    "    assert d[9] == tenthline\n",
    "\n",
    "    \n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Answer\n",
    "\n",
    "Notes:\n",
    "* Should end up with 10 dicts (not including header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# strings for directory and data file\n",
    "DATADIR = \"supporting-files/\"\n",
    "DATAFILE = \"beatles-diskography.csv\"\n",
    "\n",
    "# function to parse the data file\n",
    "def parse_file(datafile):\n",
    "    # an empty list that will be filled up\n",
    "    data = []\n",
    "    # here, \"rb\" indicates open in read binary mode\n",
    "    with open(datafile, \"r\") as f:\n",
    "        # .readline() is a Python method to read single line from file\n",
    "        # .split() returns a list of words in a string\n",
    "        # so header is a list of the words from the first line (which is the header)\n",
    "        header = f.readline().split(\",\")\n",
    "        # counter is for my for-loop\n",
    "        counter = 0\n",
    "        for line in f:\n",
    "            if counter == 10:\n",
    "                break\n",
    "            \n",
    "            fields = line.split(\",\")\n",
    "            entry = {}\n",
    "            \n",
    "            # enumerate returns an enumerate object which holds a tuple of a count plus the object being enumerated\n",
    "            for i, value in enumerate(fields):\n",
    "                # .strip() returns a copy of the string with the leading and trailing characters removed\n",
    "                entry[header[i].strip()] = value.strip()\n",
    "                \n",
    "            data.append(entry)\n",
    "            counter += 1\n",
    "            \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "    # a simple test of your implemetation\n",
    "    datafile = os.path.join(DATADIR, DATAFILE)\n",
    "    d = parse_file(datafile)\n",
    "    firstline = {'Title': 'Please Please Me', 'UK Chart Position': '1', 'Label': 'Parlophone(UK)', 'Released': '22 March 1963', 'US Chart Position': '-', 'RIAA Certification': 'Platinum', 'BPI Certification': 'Gold'}\n",
    "    tenthline = {'Title': '', 'UK Chart Position': '1', 'Label': 'Parlophone(UK)', 'Released': '10 July 1964', 'US Chart Position': '-', 'RIAA Certification': '', 'BPI Certification': 'Gold'}\n",
    "\n",
    "    assert d[0] == firstline\n",
    "    assert d[9] == tenthline\n",
    "\n",
    "    \n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro to XLRD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import dec\n",
    "import xlrd\n",
    "\n",
    "datafile = \"supporting-files/2013_ERCOT_Hourly_Load_Data.xls\"\n",
    "\n",
    "\n",
    "def parse_file(datafile):\n",
    "    workbook = xlrd.open_workbook(datafile)\n",
    "    sheet = workbook.sheet_by_index(0)\n",
    "    \n",
    "    # A list of lists where first list is headers, all other lists are the rows\n",
    "    # .cell_value is a xlrd.sheet method. Returns value of cell in given row and column\n",
    "    # List comprehension [item for other_item in looping_list]\n",
    "    # Here, item is [sheet.cell_value(r, col) for col in range(sheet.ncols)] and becomes a list with each loop\n",
    "    # The loop is for r in range(sheet.nrows)\n",
    "    # Basically, it's a way to iterate over each cell in the spreadsheet \n",
    "    # In sheet.cell_value(r, col), r and col correspond to a 2D matrix\n",
    "    # Using LC, I can move over each cell in the matrix which is added to the data list as a list\n",
    "    # r is filled first, then col is filled a number of times(range(sheet.ncols)) for each r\n",
    "        # like [[0,0 0,1 0,2 ...], [1,0 1,1 1,2 ...], ...]\n",
    "    # grab a row index, grab a column index, after each loop, create an item for the list via sheet.cell_value()\n",
    "    data = [[sheet.cell_value(r, col) for col in range(sheet.ncols)] \n",
    "                for r in range(sheet.nrows)]\n",
    "\n",
    "    print (\"\\nList Comprehension\")\n",
    "    print (\"data[3][2]:\",)\n",
    "    print (data[3][2])\n",
    "\n",
    "    print (\"\\nCells in a nested loop:\")\n",
    "    # this loop simply grabs the 50th row and prints it\n",
    "    for row in range(sheet.nrows):\n",
    "        for col in range(sheet.ncols):\n",
    "            if row == 50:\n",
    "                print(sheet.cell_value(row, col),)\n",
    "\n",
    "    ### other useful methods:\n",
    "    print (\"\\nROWS, COLUMNS, and CELLS:\")\n",
    "    print (\"Number of rows in the sheet:\",)\n",
    "    print (sheet.nrows)\n",
    "    print (\"Type of data in cell (row 3, col 2):\",)\n",
    "    print (sheet.cell_type(3, 2))\n",
    "    print (\"Value in cell (row 3, col 2):\",)\n",
    "    print (sheet.cell_value(3, 2))\n",
    "    print (\"Get a slice of values in column 3, from rows 1-3:\")\n",
    "    print (sheet.col_values(3, start_rowx=1, end_rowx=4))\n",
    "\n",
    "    print (\"\\nDATES:\")\n",
    "    print (\"Type of data in cell (row 1, col 0):\",)\n",
    "    print (sheet.cell_type(1, 0))\n",
    "    exceltime = sheet.cell_value(1, 0)\n",
    "    print (\"Time in Excel format:\",)\n",
    "    print (exceltime)\n",
    "    print (\"Convert time to a Python datetime tuple, from the Excel float:\",)\n",
    "    print (xlrd.xldate_as_tuple(exceltime, 0))\n",
    "\n",
    "    return data\n",
    "\n",
    "data = parse_file(datafile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "Your task is as follows:\n",
    "- read the provided Excel file\n",
    "# should be in memory as some kind of dataframe or list or dict\n",
    "- find and return the min, max and average values for the COAST region\n",
    "- find and return the time value for the min and max entries\n",
    "- the time values should be returned as Python tuples\n",
    "\n",
    "Please see the test function for the expected return format\n",
    "\"\"\"\n",
    "\n",
    "import xlrd\n",
    "from zipfile import ZipFile\n",
    "datafile = \"supporting-files/2013_ERCOT_Hourly_Load_Data.zip\"\n",
    "\n",
    "def open_zip(datafile):\n",
    "    with ZipFile('{0}.zip'.format(datafile), 'r') as myzip:\n",
    "        myzip.extractall()\n",
    "\n",
    "\n",
    "def parse_file(datafile):\n",
    "    workbook = xlrd.open_workbook(datafile)\n",
    "    sheet = workbook.sheet_by_index(0)\n",
    "\n",
    "    ### example on how you can get the data\n",
    "    #sheet_data = [[sheet.cell_value(r, col) for col in range(sheet.ncols)] for r in range(sheet.nrows)]\n",
    "\n",
    "    ### other useful methods:\n",
    "    # print \"\\nROWS, COLUMNS, and CELLS:\"\n",
    "    # print \"Number of rows in the sheet:\", \n",
    "    # print sheet.nrows\n",
    "    # print \"Type of data in cell (row 3, col 2):\", \n",
    "    # print sheet.cell_type(3, 2)\n",
    "    # print \"Value in cell (row 3, col 2):\", \n",
    "    # print sheet.cell_value(3, 2)\n",
    "    # print \"Get a slice of values in column 3, from rows 1-3:\"\n",
    "    # print sheet.col_values(3, start_rowx=1, end_rowx=4)\n",
    "\n",
    "    # print \"\\nDATES:\"\n",
    "    # print \"Type of data in cell (row 1, col 0):\", \n",
    "    # print sheet.cell_type(1, 0)\n",
    "    # exceltime = sheet.cell_value(1, 0)\n",
    "    # print \"Time in Excel format:\",\n",
    "    # print exceltime\n",
    "    # print \"Convert time to a Python datetime tuple, from the Excel float:\",\n",
    "    # print xlrd.xldate_as_tuple(exceltime, 0)\n",
    "    \n",
    "    \n",
    "    data = {\n",
    "            'maxtime': (0, 0, 0, 0, 0, 0),\n",
    "            'maxvalue': 0,\n",
    "            'mintime': (0, 0, 0, 0, 0, 0),\n",
    "            'minvalue': 0,\n",
    "            'avgcoast': 0\n",
    "    }\n",
    "    return data\n",
    "\n",
    "\n",
    "def test():\n",
    "    open_zip(datafile)\n",
    "    data = parse_file(datafile)\n",
    "\n",
    "    assert data['maxtime'] == (2013, 8, 13, 17, 0, 0)\n",
    "    assert round(data['maxvalue'], 10) == round(18779.02551, 10)\n",
    "\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### JSON Playground"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Use XML to obtain a JSON object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import declarations\n",
    "import json\n",
    "import requests\n",
    "\n",
    "\n",
    "BASE_URL = \"http://musicbrainz.org/ws/2/\"\n",
    "ARTIST_URL = BASE_URL + \"artist/\"\n",
    "\n",
    "# query parameters are given to the requests.get function as a dictionary\n",
    "# this variable contains some starter parameters, and is a dict with some nested dicts\n",
    "# Q: How to know what query parameters to use as key and values?\n",
    "# Q: why are some a nested dict?\n",
    "query_type = {  \"simple\": {},\n",
    "                \"atr\": {\"inc\": \"aliases+tags+ratings\"},\n",
    "                \"aliases\": {\"inc\": \"aliases\"},\n",
    "                \"releases\": {\"inc\": \"releases\"}}\n",
    "\n",
    "# This is the main function for making queries to the musicbrainz API.\n",
    "# Return type is a json object\n",
    "    # \n",
    "\n",
    "# note, all params with = are optional\n",
    "# url: of type string\n",
    "# params: of type dict\n",
    "# uid=\"\": initialized and optional - of type string\n",
    "# fmt=\"json\": initialized and optional - of type string\n",
    "def query_site(url, params, uid=\"\", fmt=\"json\"):\n",
    "    # appends params with ('fmt':'json'), which here is a nested dict from the query_type dict\n",
    "    # params comes in the the proper dict value (which is a nested dict)\n",
    "    # so this creates a new dict which the requests module knows how to handle:\n",
    "        # {\"inc\":\"releases\", \"fmt\":\"json\"}\n",
    "    params[\"fmt\"] = fmt\n",
    "    \n",
    "    # requests.get sends a GET request, returns a requests.Response object.\n",
    "    # a Response object contains a server's response to an HTTP request\n",
    "    #Input params for requests.get are:\n",
    "        # url: a url string\n",
    "        # params: a dict to also be sent in the request\n",
    "    # in this case:\n",
    "        # url is ARTIST_URL concatenated with uid, which is \"\" or whatever new string is passed\n",
    "        # params is a dict from query type ({\"inc\": \"releases\"})\n",
    "            # this dict will be sent in the query string for the request\n",
    "            # note, the requests package handles formatting the url string for me, so it handles the dict properly\n",
    "    r = requests.get(url + uid, params=params)\n",
    "    \n",
    "    # r is now a requests.Response object - so it's properties can be accessed\n",
    "    print(\"Requesting From URL: \", r.url) \n",
    "\n",
    "    # return JSON object if exists, or throw error if not\n",
    "    if r.status_code == requests.codes.ok:\n",
    "        return r.json()\n",
    "    else:\n",
    "        r.raise_for_status()\n",
    "\n",
    "\n",
    "def query_by_name(url, params, name):\n",
    "    # This adds an artist name to the query parameters before making\n",
    "    # an API call to the function above.\n",
    "    params[\"query\"] = \"artist:\" + name\n",
    "    return query_site(url, params)\n",
    "\n",
    "\n",
    "def pretty_print(data, indent=4):\n",
    "    # After we get our output, we can format it to be more readable\n",
    "    # by using this function.\n",
    "    if type(data) == dict:\n",
    "        print(json.dumps(data, indent=indent, sort_keys=True)) \n",
    "    else:\n",
    "        print(data) \n",
    "\n",
    "\n",
    "def main():\n",
    "    '''\n",
    "    Modify the function calls and indexing below to answer the questions on\n",
    "    the next quiz. HINT: Note how the output we get from the site is a\n",
    "    multi-level JSON document, so try making print statements to step through\n",
    "    the structure one level at a time or copy the output to a separate output\n",
    "    file.\n",
    "    '''\n",
    "    results = query_by_name(ARTIST_URL, query_type[\"simple\"], \"First And Kit\")\n",
    "    pretty_print(results)\n",
    "\n",
    "    artist_id = results[\"artists\"][1][\"id\"]\n",
    "    print(\"\\nARTIST:\") \n",
    "    pretty_print(results[\"artists\"][1])\n",
    "    print(\"RELEASES: \", len(results))\n",
    "\n",
    "    artist_data = query_site(ARTIST_URL, query_type[\"releases\"], artist_id)\n",
    "    releases = artist_data[\"releases\"]\n",
    "    print(\"\\nONE RELEASE:\") \n",
    "    pretty_print(releases[0], indent=2)\n",
    "    release_titles = [r[\"title\"] for r in releases]\n",
    "\n",
    "    print(\"\\nALL TITLES:\") \n",
    "    for t in release_titles:\n",
    "        print(t)\n",
    "\n",
    "\n",
    "# this is the top-level code block, first to be executed after import declarations\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Problem Set: Using CSV Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Your task is to process the supplied file and use the csv module to extract data from it.\n",
    "The data comes from NREL (National Renewable Energy Laboratory) website. Each file\n",
    "contains information from one meteorological station, in particular - about amount of\n",
    "solar and wind energy for each hour of day.\n",
    "\n",
    "Note that the first line of the datafile is neither data entry, nor header. It is a line\n",
    "describing the data source. You should extract the name of the station from it.\n",
    "\n",
    "The data should be returned as a list of lists (not dictionaries).\n",
    "You can use the csv modules \"reader\" method to get data in such format.\n",
    "Another useful method is next() - to get the next line from the iterator.\n",
    "You should only change the parse_file function.\n",
    "\"\"\"\n",
    "import csv\n",
    "import os\n",
    "\n",
    "DATADIR = \"\"\n",
    "DATAFILE = \"supporting-files/745090.csv\"\n",
    "\n",
    "\n",
    "def parse_file(datafile):\n",
    "    name = \"\"\n",
    "    data = []\n",
    "    with open(datafile,'r') as f:\n",
    "        header = f.readline().split(',')\n",
    "        name = header[1]\n",
    "        \n",
    "        # create in_reader object\n",
    "        in_reader = csv.reader(f)\n",
    "        \n",
    "        # write data from in_reader to data list\n",
    "        next(in_reader)\n",
    "        for i in in_reader:\n",
    "            data.append(i)\n",
    "    # Do not change the line below\n",
    "    name = name[1:-1]\n",
    "    return (name, data)\n",
    "\n",
    "parse_file(DATAFILE)\n",
    "\n",
    "def test():\n",
    "    datafile = os.path.join(DATADIR, DATAFILE)\n",
    "    name, data = parse_file(datafile)\n",
    "\n",
    "    assert name == \"MOUNTAIN VIEW MOFFETT FLD NAS\"\n",
    "    assert data[0][1] == \"01:00\"\n",
    "    assert data[2][0] == \"01/01/2005\"\n",
    "    assert data[2][5] == \"2\"\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Problem Set: Excel to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Algo:\n",
    "LOAD FILE INTO\n",
    "EXPORT AS CSV '''\n",
    "# -*- coding: utf-8 -*-\n",
    "'''\n",
    "Find the time and value of max load for each of the regions\n",
    "COAST, EAST, FAR_WEST, NORTH, NORTH_C, SOUTHERN, SOUTH_C, WEST\n",
    "and write the result out in a csv file, using pipe character | as the delimiter.\n",
    "\n",
    "An example output can be seen in the \"example.csv\" file.\n",
    "'''\n",
    "\n",
    "import xlrd\n",
    "import os\n",
    "import csv\n",
    "from zipfile import ZipFile\n",
    "\n",
    "datafile = \"2013_ERCOT_Hourly_Load_Data.xls\"\n",
    "outfile = \"2013_Max_Loads.csv\"\n",
    "\n",
    "\n",
    "def open_zip(datafile):\n",
    "    with ZipFile('{0}.zip'.format(datafile), 'r') as myzip:\n",
    "        myzip.extractall()\n",
    "\n",
    "\n",
    "def parse_file(datafile):\n",
    "    workbook = xlrd.open_workbook(datafile)\n",
    "    sheet = workbook.sheet_by_index(0)\n",
    "    data = None\n",
    "    # YOUR CODE HERE\n",
    "    # Remember that you can use xlrd.xldate_as_tuple(sometime, 0) to convert\n",
    "    # Excel date to Python tuple of (year, month, day, hour, minute, second)\n",
    "    return data\n",
    "\n",
    "def save_file(data, filename):\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    \n",
    "def test():\n",
    "    open_zip(datafile)\n",
    "    data = parse_file(datafile)\n",
    "    save_file(data, outfile)\n",
    "\n",
    "    number_of_rows = 0\n",
    "    stations = []\n",
    "\n",
    "    ans = {'FAR_WEST': {'Max Load': '2281.2722140000024',\n",
    "                        'Year': '2013',\n",
    "                        'Month': '6',\n",
    "                        'Day': '26',\n",
    "                        'Hour': '17'}}\n",
    "    correct_stations = ['COAST', 'EAST', 'FAR_WEST', 'NORTH',\n",
    "                        'NORTH_C', 'SOUTHERN', 'SOUTH_C', 'WEST']\n",
    "    fields = ['Year', 'Month', 'Day', 'Hour', 'Max Load']\n",
    "\n",
    "    with open(outfile) as of:\n",
    "        csvfile = csv.DictReader(of, delimiter=\"|\")\n",
    "        for line in csvfile:\n",
    "            station = line['Station']\n",
    "            if station == 'FAR_WEST':\n",
    "                for field in fields:\n",
    "                    # Check if 'Max Load' is within .1 of answer\n",
    "                    if field == 'Max Load':\n",
    "                        max_answer = round(float(ans[station][field]), 1)\n",
    "                        max_line = round(float(line[field]), 1)\n",
    "                        assert max_answer == max_line\n",
    "\n",
    "                    # Otherwise check for equality\n",
    "                    else:\n",
    "                        assert ans[station][field] == line[field]\n",
    "\n",
    "            number_of_rows += 1\n",
    "            stations.append(station)\n",
    "\n",
    "        # Output should be 8 lines not including header\n",
    "        assert number_of_rows == 8\n",
    "\n",
    "        # Check Station Names\n",
    "        assert set(stations) == set(correct_stations)\n",
    "\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Data from XML Quiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# Your task here is to extract data from xml on authors of an article\n",
    "# and add it to a list, one item for an author.\n",
    "# See the provided data structure for the expected format.\n",
    "# The tags for first name, surname and email should map directly\n",
    "# to the dictionary keys\n",
    "\n",
    "# note, format is a list of dicts\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "article_file = \"supporting-files/exampleResearchArticle.xml\"\n",
    "\n",
    "\n",
    "def get_root(fname):\n",
    "    tree = ET.parse(fname)\n",
    "    return tree.getroot()\n",
    "\n",
    "\n",
    "# function takes in a root object\n",
    "def get_authors(root):\n",
    "    # list holds the dicts\n",
    "    authors = []\n",
    "    \n",
    "    # root.findall contains all the objects of au tag\n",
    "    for author in root.findall('./fm/bibl/aug/au'):\n",
    "        # with each loop, data will be \n",
    "        data = {\n",
    "                \"fnm\": None,\n",
    "                \"snm\": None,\n",
    "                \"email\": None\n",
    "        }\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        data['fnm'] = author.find('fnm').text\n",
    "        data['snm'] = author.find('snm').text\n",
    "        data['email'] = author.find('email').text\n",
    "\n",
    "        authors.append(data)\n",
    "\n",
    "    return authors\n",
    "\n",
    "\n",
    "def test():\n",
    "    solution = [{'fnm': 'Omer', 'snm': 'Mei-Dan', 'email': 'omer@extremegate.com'}, {'fnm': 'Mike', 'snm': 'Carmont', 'email': 'mcarmont@hotmail.com'}, {'fnm': 'Lior', 'snm': 'Laver', 'email': 'laver17@gmail.com'}, {'fnm': 'Meir', 'snm': 'Nyska', 'email': 'nyska@internet-zahav.net'}, {'fnm': 'Hagay', 'snm': 'Kammar', 'email': 'kammarh@gmail.com'}, {'fnm': 'Gideon', 'snm': 'Mann', 'email': 'gideon.mann.md@gmail.com'}, {'fnm': 'Barnaby', 'snm': 'Clarck', 'email': 'barns.nz@gmail.com'}, {'fnm': 'Eugene', 'snm': 'Kots', 'email': 'eukots@gmail.com'}]\n",
    "    \n",
    "    # root loads the root object\n",
    "    root = get_root(article_file)\n",
    "    # data takes the root object in as an input param\n",
    "    data = get_authors(root)\n",
    "\n",
    "    assert data[0] == solution[0]\n",
    "    assert data[1][\"fnm\"] == solution[1][\"fnm\"]\n",
    "\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### NEED: complete XML and JSON challenges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import pprint\n",
    "\n",
    "tree = ET.parse('supporting-files/exampleresearcharticle.xml')\n",
    "root = tree.getroot()\n",
    "\n",
    "print(\"\\nChildren of root\")\n",
    "for child in root:\n",
    "    print(child.tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrangling JSON Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling Attributes Quiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# Your task here is to extract data from xml on authors of an article\n",
    "# and add it to a list, one item for an author.\n",
    "# See the provided data structure for the expected format.\n",
    "# The tags for first name, surname and email should map directly\n",
    "# to the dictionary keys, but you have to extract the attributes from the \"insr\" tag\n",
    "# and add them to the list for the dictionary key \"insr\"\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "article_file = \"supporting-files/exampleResearchArticle.xml\"\n",
    "\n",
    "\n",
    "def get_root(fname):\n",
    "    tree = ET.parse(fname)\n",
    "    return tree.getroot()\n",
    "\n",
    "\n",
    "def get_authors(root):\n",
    "    authors = []\n",
    "    for author in root.findall('./fm/bibl/aug/au'):\n",
    "        data = {\n",
    "                \"fnm\": None,\n",
    "                \"snm\": None,\n",
    "                \"email\": None,\n",
    "                \"insr\": []\n",
    "        }\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        data['fnm'] = author.find('fnm').text\n",
    "        data['snm'] = author.find('snm').text\n",
    "        data['email'] = author.find('email').text\n",
    "        insr = author.findall('./insr')\n",
    "        for i in insr:\n",
    "            data['insr'].append(i.attrib['iid'])\n",
    "\n",
    "        authors.append(data)\n",
    "\n",
    "    return authors\n",
    "\n",
    "\n",
    "def test():\n",
    "    solution = [{'insr': ['I1'], 'fnm': 'Omer', 'snm': 'Mei-Dan', 'email': 'omer@extremegate.com'},\n",
    "                {'insr': ['I2'], 'fnm': 'Mike', 'snm': 'Carmont', 'email': 'mcarmont@hotmail.com'},\n",
    "                {'insr': ['I3', 'I4'], 'fnm': 'Lior', 'snm': 'Laver', 'email': 'laver17@gmail.com'},\n",
    "                {'insr': ['I3'], 'fnm': 'Meir', 'snm': 'Nyska', 'email': 'nyska@internet-zahav.net'},\n",
    "                {'insr': ['I8'], 'fnm': 'Hagay', 'snm': 'Kammar', 'email': 'kammarh@gmail.com'},\n",
    "                {'insr': ['I3', 'I5'], 'fnm': 'Gideon', 'snm': 'Mann', 'email': 'gideon.mann.md@gmail.com'},\n",
    "                {'insr': ['I6'], 'fnm': 'Barnaby', 'snm': 'Clarck', 'email': 'barns.nz@gmail.com'},\n",
    "                {'insr': ['I7'], 'fnm': 'Eugene', 'snm': 'Kots', 'email': 'eukots@gmail.com'}]\n",
    "\n",
    "    root = get_root(article_file)\n",
    "    data = get_authors(root)\n",
    "\n",
    "    assert data[0] == solution[0]\n",
    "    assert data[1][\"insr\"] == solution[1][\"insr\"]\n",
    "\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# Please note that the function 'make_request' is provided for your reference only.\n",
    "# You will not be able to to actually use it from within the Udacity web UI.\n",
    "# Your task is to process the HTML using BeautifulSoup, extract the hidden\n",
    "# form field values for \"__EVENTVALIDATION\" and \"__VIEWSTATE\" and set the appropriate\n",
    "# values in the data dictionary.\n",
    "# All your changes should be in the 'extract_data' function\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "\n",
    "html_page = \"supporting-files/page_source.html\"\n",
    "\n",
    "\n",
    "def extract_data(page):\n",
    "    data = {\"eventvalidation\": \"\",\n",
    "            \"viewstate\": \"\"}\n",
    "    with open(page, \"r\") as html:\n",
    "        # do something here to find the necessary values\n",
    "        pass\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def make_request(data):\n",
    "    eventvalidation = data[\"eventvalidation\"]\n",
    "    viewstate = data[\"viewstate\"]\n",
    "\n",
    "    r = requests.post(\"http://www.transtats.bts.gov/Data_Elements.aspx?Data=2\",\n",
    "                    data={'AirportList': \"BOS\",\n",
    "                          'CarrierList': \"VX\",\n",
    "                          'Submit': 'Submit',\n",
    "                          \"__EVENTTARGET\": \"\",\n",
    "                          \"__EVENTARGUMENT\": \"\",\n",
    "                          \"__EVENTVALIDATION\": eventvalidation,\n",
    "                          \"__VIEWSTATE\": viewstate\n",
    "                    })\n",
    "\n",
    "    return r.text\n",
    "\n",
    "\n",
    "def test():\n",
    "    data = extract_data(html_page)\n",
    "    assert data[\"eventvalidation\"] != \"\"\n",
    "    assert data[\"eventvalidation\"].startswith(\"/wEWjAkCoIj1ng0\")\n",
    "    assert data[\"viewstate\"].startswith(\"/wEPDwUKLTI\")\n",
    "\n",
    "    \n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Hidden phrase 1:\n",
    "You got it!\n",
    "\n",
    "\n",
    "Here's your first HIDDEN PHRASE.\n",
    "\n",
    "\n",
    "HIDDEN PHRASE 1 of 3: \n",
    "\n",
    "\n",
    "\"SELECT sql, statement FROM Udacious WHERE queryId = 35;\"\n",
    "\n",
    " After unlocking all the phrases you'll be ready to collect your gift at the end of the course :)\n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    "THAT IS CORRECT!!\n",
    "\n",
    "\n",
    "Here's your first HIDDEN PHRASE.\n",
    "\n",
    "\n",
    "HIDDEN PHRASE 2 of 3: \n",
    "\n",
    "\n",
    "Table Name:\n",
    "Udacious \n",
    "\n",
    "\n",
    "Columns:\n",
    "problemSet INTEGER, node INTEGER,  queryId INTEGER, title TEXT, sql TEXT, statement TEXT\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "WOW! You made it...Congratulations on all of you hard work.\n",
    "\n",
    "\n",
    "HIDDEN MESSAGE #3 = Awesome.db\n",
    "\n",
    "\n",
    "Head to the Gift page and plug in your hidden messages!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "animals\n",
    "This table lists individual animals in the zoo. Each animal has only one row. There may be multiple animals with the same name, or even multiple animals with the same name and species.\n",
    "name — the animal's name (example: 'George')\n",
    "species — the animal's species (example: 'gorilla')\n",
    "birthdate — the animal's date of birth (example: '1998-05-18')\n",
    "diet\n",
    "This table matches up species with the foods they eat. Every species in the zoo eats at least one sort of food, and many eat more than one. If a species eats more than one food, there will be more than one row for that species.\n",
    "species — the name of a species (example: 'hyena')\n",
    "food — the name of a food that species eats (example: 'meat')\n",
    "taxonomy\n",
    "This table gives the (partial) biological taxonomic names for each species in the zoo. It can be used to find which species are more closely related to each other evolutionarily.\n",
    "name — the common name of the species (e.g. 'jackal')\n",
    "species — the taxonomic species name (e.g. 'aureus')\n",
    "genus — the taxonomic genus name (e.g. 'Canis')\n",
    "family — the taxonomic family name (e.g. 'Canidae')\n",
    "t_order — the taxonomic order name (e.g. 'Carnivora')\n",
    "If you've never heard of this classification, don't worry about it; the details won't be necessary for this course. But if you're curious, Wikipedia articles Taxonomy and Biological classification may help.\n",
    "\n",
    "ordernames\n",
    "This table gives the common names for each of the taxonomic orders in the taxonomy table.\n",
    "t_order — the taxonomic order name (e.g. 'Cetacea')\n",
    "name — the common name (e.g. 'whales and dolphins')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#\n",
    "# Uncomment one of these QUERY variables at a time by deleting the #.\n",
    "# Use \"Test Run\" to run it.\n",
    "# You'll see the results below.  Then try your own queries as well!\n",
    "#\n",
    "\n",
    "QUERY = \"select max(name) from animals;\"\n",
    "\n",
    "QUERY = \"select * from animals limit 10;\"\n",
    "\n",
    "QUERY = \"select * from animals where species = 'orangutan' order by birthdate;\"\n",
    "\n",
    "#QUERY = \"select name from animals where species = 'orangutan' order by birthdate desc;\"\n",
    "\n",
    "#QUERY = \"select name, birthdate from animals order by name limit 10 offset 20;\"\n",
    "\n",
    "#QUERY = \"select species, min(birthdate) from animals group by species;\"\n",
    "\n",
    "#QUERY = '''\n",
    "#select name, count(*) as num from animals\n",
    "#group by name\n",
    "#order by num desc\n",
    "#limit 5;\n",
    "#'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "SELECT Email, FirstName, LastName, Total FROM Customer LEFT JOIN Invoice ON Customer.CustomerId = Invoice.CustomerId;\n",
    "\n",
    "SELECT CustomerId, SUM(Total) FROM Invoice GROUP BY CustomerId;\n",
    "\n",
    "SELECT Customer.CustomerId, Customer.Email, Customer.FirstName, Customer.LastName, SUM(Invoice.Total) AS Total FROM Customer LEFT JOIN Invoice ON Customer.CustomerId = Invoice.CustomerId GROUP BY Customer.CustomerId;\n",
    "\n",
    "SELECT Invoice.CustomerId, Customer.Email, Customer.FirstName, Customer.LastName, SUM(Total) AS Total FROM Invoice LEFT JOIN Customer On Invoice.CustomerId = Customer.CustomerId GROUP BY FirstName;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##  Rock Music Lives on!  After the success of your recent email campaign,  \n",
    "##  you're interested in targeting your long standing Rock Music audience!\n",
    "##  You'll need to collect a list of emails containing each of your Rock Music listeners.\n",
    "\n",
    "##  Use your query to return the email, first name, last name, and Genre of all Rock Music listeners!\n",
    "##  Return you list ordered alphabetically by email address starting with A.\n",
    "##  Can you find a way to deal with duplicate email addresses so no one receives multiple emails?\n",
    "\n",
    "\n",
    "QUERY ='''\n",
    "SELECT ...\n",
    "'''\n",
    "\n",
    "'''\n",
    "---VISUAL GUIDE---\n",
    "\n",
    "Before query...\n",
    "\n",
    "##############     ###############     #################     ############      ###########\n",
    "#  Customer  #     #  Invoice    #     #  InvoiceLine  #     #  Track   #      #  Genre  # \n",
    "##############     ###############     #################     ############      ###########\n",
    "| CustomerId | --> | CustomerId  |     |  TrackId      | --> | TrackId  |      |  Name   |\n",
    "+============+     +=============+     +===============+     +==========+      +=========+\n",
    "|  Email     |     |  InvoiceId  | --> |  InvoiceId    |     | GenreId  | -->  | GenreId |\n",
    "+============+     +=============+     +===============+     +==========+      +=========+\n",
    "|  FirstName |                                                  \n",
    "+============+\n",
    "|  LastName  |                                                              \n",
    "+============+\n",
    "\n",
    "After query...\n",
    "\n",
    "###############################################\n",
    "#                 CustomerGenre               #   <-----RESULT!\n",
    "###############################################\n",
    "|  Email  |  FirstName  |  LastName  | Genre  |\n",
    "+=========+=============+============+========+\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Challenges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# command = '''\n",
    "# SELECT \n",
    "# Customer.Email, \n",
    "# Customer.FirstName, \n",
    "# Customer.LastName, \n",
    "# Genre.Name AS Genre \n",
    "# FROM Customer \n",
    "#     JOIN Invoice \n",
    "#         ON Customer.CustomerId=Invoice.CustomerId\n",
    "#     JOIN InvoiceLine \n",
    "#         ON Invoice.InvoiceId=InvoiceLine.InvoiceId\n",
    "#     JOIN Track\n",
    "#         ON InvoiceLine.TrackId=Track.TrackId \n",
    "#     JOIN Genre\n",
    "#         ON Track.GenreId=Genre.GenreId\n",
    "# WHERE Genre.GenreId = 1 \n",
    "# GROUP BY Customer.CustomerId \n",
    "# ORDER BY Customer.Email ASC\n",
    "# '''\n",
    "\n",
    "\n",
    "# command = '''\n",
    "# SELECT \n",
    "# BillingCity, SUM(Total ) \n",
    "# FROM Invoice \n",
    "# GROUP BY BillingCity \n",
    "# ORDER BY SUM(Total) DESC\n",
    "# limit 1\n",
    "# '''\n",
    "\n",
    "\n",
    "# command = '''\n",
    "# SELECT \n",
    "# Invoice.BillingCity,\n",
    "# COUNT(Genre.Name),\n",
    "# Genre.Name\n",
    "# FROM Invoice \n",
    "#     JOIN InvoiceLine\n",
    "#         ON Invoice.InvoiceId=InvoiceLine.InvoiceId\n",
    "#     JOIN Track\n",
    "#         ON InvoiceLine.TrackId=Track.TrackID\n",
    "#     JOIN Genre\n",
    "#         ON Track.GenreId=Genre.GenreId\n",
    "# WHERE Invoice.BillingCity = 'Prague'\n",
    "# GROUP BY Genre.Name\n",
    "# ORDER BY COUNT(Genre.Name) DESC\n",
    "# limit 3\n",
    "# '''\n",
    "\n",
    "\n",
    "# command = '''\n",
    "# SELECT \n",
    "# Artist.Name as Artist, \n",
    "# COUNT(Genre.Name) as count \n",
    "# FROM Genre\n",
    "#     JOIN Track\n",
    "#         ON Genre.GenreId=Track.GenreId\n",
    "#     JOIN Album\n",
    "#         ON Track.AlbumId=Album.AlbumId\n",
    "#     JOIN Artist\n",
    "#         ON Album.ArtistId=Artist.ArtistId\n",
    "# WHERE Genre.GenreId = 1\n",
    "# GROUP BY Artist.Name\n",
    "# ORDER BY COUNT(Genre.Name) DESC\n",
    "# limit 10\n",
    "# '''\n",
    "\n",
    "\n",
    "# command = '''\n",
    "# SELECT\n",
    "# Invoice.BillingCity,\n",
    "# Count(Genre.Name) AS NumTracks\n",
    "# FROM Invoice\n",
    "#     JOIN InvoiceLine\n",
    "#         ON Invoice.InvoiceId=InvoiceLine.InvoiceId\n",
    "#     JOIN Track\n",
    "#         ON InvoiceLine.TrackId=Track.TrackId\n",
    "#     JOIN Genre\n",
    "#         ON Track.GenreId=Genre.GenreId\n",
    "# WHERE Genre.GenreId = 4 and Invoice.BillingCountry = 'France'\n",
    "# GROUP BY Invoice.BillingCity\n",
    "# ORDER BY NumTracks DESC\n",
    "# '''\n",
    "\n",
    "\n",
    "# command = '''\n",
    "# SELECT SUM(Total)\n",
    "# FROM \n",
    "# (SELECT COUNT(*) AS Total\n",
    "# FROM Invoice\n",
    "# GROUP BY BillingCountry\n",
    "# ORDER BY Total DESC\n",
    "# LIMIT 5);\n",
    "# '''\n",
    "\n",
    "\n",
    "# command = '''\n",
    "# SELECT BillingCity, BillingState, BillingCountry, Total\n",
    "# FROM Invoice,\n",
    "# (SELECT AVG(Total) AS Average\n",
    "# FROM Invoice) as Subquery\n",
    "# WHERE Total > average;\n",
    "# '''\n",
    "\n",
    "\n",
    "# command = '''\n",
    "# SELECT FirstName, LastName, BillingCity, BillingState, BillingCountry, Total\n",
    "# FROM Invoice\n",
    "#     JOIN Customer\n",
    "#         JOIN (SELECT avg(Total) AS Average FROM Invoice) AS Subquery\n",
    "# WHERE Total > Average;\n",
    "# '''\n",
    "\n",
    "\n",
    "# command = '''\n",
    "# SELECT FirstName, LastName, BillingCity, BillingState, BillingCountry, Total\n",
    "# FROM Invoice\n",
    "#     JOIN Customer\n",
    "#         JOIN (SELECT avg(Total) AS Average FROM Invoice) AS Subquery\n",
    "# WHERE Total > Average;\n",
    "# '''\n",
    "\n",
    "\n",
    "# command = '''\n",
    "# SELECT COUNT(DISTINCT Invoice.CustomerId), Invoice.CustomerId, Invoice.InvoiceId, InvoiceLine.InvoiceLineId, InvoiceLine.TrackId, Track.GenreId\n",
    "# FROM Invoice\n",
    "#     JOIN InvoiceLine\n",
    "#         ON Invoice.InvoiceId=InvoiceLine.InvoiceId\n",
    "#     JOIN Track\n",
    "#         ON InvoiceLine.TrackId=Track.TrackId\n",
    "# WHERE Track.GenreId=2\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "# stop truncated columns on a DataFrame\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 6)\n",
    "\n",
    "conn = sqlite3.connect('supporting-files/chinook.db')\n",
    "c = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x111eb0f80>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# query to find average song length:\n",
    "# SELECT AVG(Track.Milliseconds) AverageSongLenth FROM Track\n",
    "\n",
    "# test vector join average song lenth\n",
    "command = '''\n",
    "SELECT *, COUNT(Track.GenreId)\n",
    "FROM Track\n",
    "    JOIN (SELECT AVG(Track.Milliseconds) AverageSongLength FROM Track) AS Subquery\n",
    "WHERE Track.Milliseconds < Subquery.AverageSongLength\n",
    "GROUP BY Track.GenreId\n",
    "'''\n",
    "\n",
    "\n",
    "# command = '''\n",
    "# SELECT BillingCity, BillingState, BillingCountry, Total\n",
    "# FROM Invoice,\n",
    "# (SELECT AVG(Total) AS Average\n",
    "# FROM Invoice) as Subquery\n",
    "# WHERE Total > average;\n",
    "# '''\n",
    "\n",
    "c.execute(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TrackId</th>\n",
       "      <th>Name</th>\n",
       "      <th>AlbumId</th>\n",
       "      <th>MediaTypeId</th>\n",
       "      <th>GenreId</th>\n",
       "      <th>Composer</th>\n",
       "      <th>Milliseconds</th>\n",
       "      <th>Bytes</th>\n",
       "      <th>UnitPrice</th>\n",
       "      <th>AverageSongLength</th>\n",
       "      <th>COUNT(Track.GenreId)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3355</td>\n",
       "      <td>Love Comes</td>\n",
       "      <td>265</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Darius \"Take One\" Minwalla/Jon Auer/Ken String...</td>\n",
       "      <td>199923</td>\n",
       "      <td>3240609</td>\n",
       "      <td>0.99</td>\n",
       "      <td>393599.212104</td>\n",
       "      <td>1162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3357</td>\n",
       "      <td>OAM's Blues</td>\n",
       "      <td>267</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>Aaron Goldberg</td>\n",
       "      <td>266936</td>\n",
       "      <td>4292028</td>\n",
       "      <td>0.99</td>\n",
       "      <td>393599.212104</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3145</td>\n",
       "      <td>Sweet Lady Luck</td>\n",
       "      <td>141</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Vandenberg</td>\n",
       "      <td>273737</td>\n",
       "      <td>8919163</td>\n",
       "      <td>0.99</td>\n",
       "      <td>393599.212104</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3478</td>\n",
       "      <td>Slowness</td>\n",
       "      <td>323</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>None</td>\n",
       "      <td>215386</td>\n",
       "      <td>3644793</td>\n",
       "      <td>0.99</td>\n",
       "      <td>393599.212104</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3502</td>\n",
       "      <td>Quintet for Horn, Violin, 2 Violas, and Cello ...</td>\n",
       "      <td>346</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>Wolfgang Amadeus Mozart</td>\n",
       "      <td>221331</td>\n",
       "      <td>3665114</td>\n",
       "      <td>0.99</td>\n",
       "      <td>393599.212104</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3451</td>\n",
       "      <td>Die Zauberflöte, K.620: \"Der Hölle Rache Kocht...</td>\n",
       "      <td>317</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>Wolfgang Amadeus Mozart</td>\n",
       "      <td>174813</td>\n",
       "      <td>2861468</td>\n",
       "      <td>0.99</td>\n",
       "      <td>393599.212104</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    TrackId                                               Name  AlbumId  \\\n",
       "0      3355                                         Love Comes      265   \n",
       "1      3357                                        OAM's Blues      267   \n",
       "2      3145                                    Sweet Lady Luck      141   \n",
       "..      ...                                                ...      ...   \n",
       "18     3478                                           Slowness      323   \n",
       "19     3502  Quintet for Horn, Violin, 2 Violas, and Cello ...      346   \n",
       "20     3451  Die Zauberflöte, K.620: \"Der Hölle Rache Kocht...      317   \n",
       "\n",
       "    MediaTypeId  GenreId                                           Composer  \\\n",
       "0             5        1  Darius \"Take One\" Minwalla/Jon Auer/Ken String...   \n",
       "1             5        2                                     Aaron Goldberg   \n",
       "2             1        3                                         Vandenberg   \n",
       "..          ...      ...                                                ...   \n",
       "18            2       23                                               None   \n",
       "19            2       24                            Wolfgang Amadeus Mozart   \n",
       "20            2       25                            Wolfgang Amadeus Mozart   \n",
       "\n",
       "    Milliseconds    Bytes  UnitPrice  AverageSongLength  COUNT(Track.GenreId)  \n",
       "0         199923  3240609       0.99      393599.212104                  1162  \n",
       "1         266936  4292028       0.99      393599.212104                   116  \n",
       "2         273737  8919163       0.99      393599.212104                   300  \n",
       "..           ...      ...        ...                ...                   ...  \n",
       "18        215386  3644793       0.99      393599.212104                    38  \n",
       "19        221331  3665114       0.99      393599.212104                    60  \n",
       "20        174813  2861468       0.99      393599.212104                     1  \n",
       "\n",
       "[21 rows x 11 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_sql(command, conn)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'COAST': {'maxval': 18779.025510000003, 'maxtime': (2013, 8, 13, 17, 0, 0)}, 'SOUTH_C': {'maxval': 11433.30491600001, 'maxtime': (2013, 8, 8, 18, 0, 0)}, 'NORTH': {'maxval': 1544.7707140000005, 'maxtime': (2013, 8, 7, 17, 0, 0)}, 'NORTH_C': {'maxval': 24415.570226999993, 'maxtime': (2013, 8, 7, 18, 0, 0)}, 'WEST': {'maxval': 1862.6137649999998, 'maxtime': (2013, 8, 7, 17, 0, 0)}, 'SOUTHERN': {'maxval': 5494.157645, 'maxtime': (2013, 8, 8, 16, 0, 0)}, 'EAST': {'maxval': 2380.1654089999956, 'maxtime': (2013, 8, 5, 17, 0, 0)}, 'FAR_WEST': {'maxval': 2281.2722140000024, 'maxtime': (2013, 6, 26, 17, 0, 0)}}\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "'''\n",
    "Find the time and value of max load for each of the regions\n",
    "COAST, EAST, FAR_WEST, NORTH, NORTH_C, SOUTHERN, SOUTH_C, WEST\n",
    "and write the result out in a csv file, using pipe character | as the delimiter.\n",
    "\n",
    "An example output can be seen in the \"example.csv\" file.\n",
    "'''\n",
    "\n",
    "import xlrd\n",
    "import os\n",
    "import csv\n",
    "# from zipfile import ZipFile\n",
    "\n",
    "datafile = \"supporting-files/2013_ERCOT_Hourly_Load_Data.xls\"\n",
    "outfile = \"2013_Max_Loads.csv\"\n",
    "\n",
    "\n",
    "# def open_zip(datafile):\n",
    "#     with ZipFile('{0}.zip'.format(datafile), 'r') as myzip:\n",
    "#         myzip.extractall()\n",
    "\n",
    "\n",
    "# parses an excel file to make a dict\n",
    "def parse_file(datafile):\n",
    "#     workbook = xlrd.open_workbook(datafile)\n",
    "    # open the workbook\n",
    "    workbook = xlrd.open_workbook(datafile)\n",
    "    \n",
    "    # open the sheet\n",
    "    sheet = workbook.sheet_by_index(0)\n",
    "    \n",
    "    # empty dict\n",
    "    data = {}\n",
    "    \n",
    "    # process all rows that contain station data\n",
    "    for n in range (1, 9):\n",
    "        # uses sheet.cell_value() function to grab the header (EAST, FAR_WEST, etc.)\n",
    "        # sheet.cell_value(row, col)\n",
    "        station = sheet.cell_value(0, n)\n",
    "        \n",
    "        # creates a list out of all the values from a column\n",
    "        cv = sheet.col_values(n, start_rowx=1, end_rowx=None)\n",
    "        \n",
    "        # find max value from the list\n",
    "        maxval = max(cv)\n",
    "        \n",
    "        # using .index function, find the index position of the max value\n",
    "        maxpos = cv.index(maxval) + 1\n",
    "        \n",
    "        # use the position of the max value to find the timestamp\n",
    "        maxtime = sheet.cell_value(maxpos, 0)\n",
    "        \n",
    "        # converts the timestamp into a tuple\n",
    "        realtime = xlrd.xldate_as_tuple(maxtime, 0)\n",
    "        \n",
    "        # dict operation to add the station name as the key\n",
    "        # and its value as a nested dict with the maximum value float and tuple of date \n",
    "        data[station] = {\"maxval\": maxval,\n",
    "                         \"maxtime\": realtime}\n",
    "\n",
    "    print(data)\n",
    "    return data\n",
    "\n",
    "# uses the dict to make a csv file\n",
    "def save_file(data, filename):\n",
    "    # open desired file name in write mode\n",
    "    with open(filename, \"w\") as f:\n",
    "        \n",
    "        # creates writer object with a delimeter\n",
    "        w = csv.writer(f, delimiter='|')\n",
    "        \n",
    "        # .writerow function to make the first row\n",
    "        w.writerow([\"Station\", \"Year\", \"Month\", \"Day\", \"Hour\", \"Max Load\"])\n",
    "        \n",
    "        \n",
    "        for s in data:\n",
    "            # unpack the tuple - note, underscores are just going to be ignored\n",
    "            year, month, day, hour, _ , _= data[s][\"maxtime\"]\n",
    "            \n",
    "            # note, s in this loop is the Station because looping over a dict\n",
    "            # loops over the keys\n",
    "            w.writerow([s, year, month, day, hour, data[s][\"maxval\"]])\n",
    "\n",
    "    \n",
    "def test():\n",
    "#     open_zip(datafile)\n",
    "    data = parse_file(datafile)\n",
    "    save_file(data, outfile)\n",
    "\n",
    "    number_of_rows = 0\n",
    "    stations = []\n",
    "\n",
    "    ans = {'FAR_WEST': {'Max Load': '2281.2722140000024',\n",
    "                        'Year': '2013',\n",
    "                        'Month': '6',\n",
    "                        'Day': '26',\n",
    "                        'Hour': '17'}}\n",
    "    correct_stations = ['COAST', 'EAST', 'FAR_WEST', 'NORTH',\n",
    "                        'NORTH_C', 'SOUTHERN', 'SOUTH_C', 'WEST']\n",
    "    fields = ['Year', 'Month', 'Day', 'Hour', 'Max Load']\n",
    "\n",
    "    with open(outfile) as of:\n",
    "        csvfile = csv.DictReader(of, delimiter=\"|\")\n",
    "        for line in csvfile:\n",
    "            station = line['Station']\n",
    "            if station == 'FAR_WEST':\n",
    "                for field in fields:\n",
    "                    # Check if 'Max Load' is within .1 of answer\n",
    "                    if field == 'Max Load':\n",
    "                        max_answer = round(float(ans[station][field]), 1)\n",
    "                        max_line = round(float(line[field]), 1)\n",
    "                        assert max_answer == max_line\n",
    "\n",
    "                    # Otherwise check for equality\n",
    "                    else:\n",
    "                        assert ans[station][field] == line[field]\n",
    "\n",
    "            number_of_rows += 1\n",
    "            stations.append(station)\n",
    "\n",
    "        # Output should be 8 lines not including header\n",
    "        assert number_of_rows == 8\n",
    "\n",
    "        # Check Station Names\n",
    "        assert set(stations) == set(correct_stations)\n",
    "\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'popular-viewed-1.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-44328f44651a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-44328f44651a>\u001b[0m in \u001b[0;36mtest\u001b[0;34m()\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m     \u001b[0mtitles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marticle_overview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"viewed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitles\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murls\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-44328f44651a>\u001b[0m in \u001b[0;36marticle_overview\u001b[0;34m(kind, period)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0marticle_overview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0mtitles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0murls\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-44328f44651a>\u001b[0m in \u001b[0;36mget_from_file\u001b[0;34m(kind, period)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"popular-{0}-{1}.json\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'popular-viewed-1.json'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Your task in this exercise is to modify 'extract_carrier()` to get a list of\n",
    "all airlines. Exclude all of the combination values like \"All U.S. Carriers\"\n",
    "from the data that you return. You should return a list of codes for the\n",
    "carriers.\n",
    "\n",
    "All your changes should be in the 'extract_carrier()' function. The\n",
    "'options.html' file in the tab above is a stripped down version of what is\n",
    "actually on the website, but should provide an example of what you should get\n",
    "from the full file.\n",
    "\n",
    "Please note that the function 'make_request()' is provided for your reference\n",
    "only. You will not be able to to actually use it from within the Udacity web UI.\n",
    "\"\"\"\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "html_page = \"options.html\"\n",
    "\n",
    "\n",
    "def extract_carriers(page):\n",
    "    data = []\n",
    "\n",
    "    with open(page, \"r\") as html:\n",
    "        # do something here to find the necessary values\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def make_request(data):\n",
    "    eventvalidation = data[\"eventvalidation\"]\n",
    "    viewstate = data[\"viewstate\"]\n",
    "    airport = data[\"airport\"]\n",
    "    carrier = data[\"carrier\"]\n",
    "\n",
    "    r = s.post(\"https://www.transtats.bts.gov/Data_Elements.aspx?Data=2\",\n",
    "               data = ((\"__EVENTTARGET\", \"\"),\n",
    "                       (\"__EVENTARGUMENT\", \"\"),\n",
    "                       (\"__VIEWSTATE\", viewstate),\n",
    "                       (\"__VIEWSTATEGENERATOR\",viewstategenerator),\n",
    "                       (\"__EVENTVALIDATION\", eventvalidation),\n",
    "                       (\"CarrierList\", carrier),\n",
    "                       (\"AirportList\", airport),\n",
    "                       (\"Submit\", \"Submit\")))\n",
    "\n",
    "    return r.text\n",
    "\n",
    "\n",
    "def test():\n",
    "    data = extract_carriers(html_page)\n",
    "    assert len(data) == 16\n",
    "    assert \"FL\" in data\n",
    "    assert \"NK\" in data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# So, the problem is that the gigantic file is actually not a valid XML, because\n",
    "# it has several root elements, and XML declarations.\n",
    "# It is, a matter of fact, a collection of a lot of concatenated XML documents.\n",
    "# So, one solution would be to split the file into separate documents,\n",
    "# so that you can process the resulting files as valid XML documents.\n",
    "\n",
    "\"\"\"\n",
    "    Split the input file into separate files, each containing a single patent.\n",
    "    As a hint - each patent declaration starts with the same line that was\n",
    "    causing the error found in the previous exercises.\n",
    "    \n",
    "    The new files should be saved with filename in the following format:\n",
    "    \"{}-{}\".format(filename, n) where n is a counter, starting from 0.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "PATENTS = 'supporting-files/patent.data'\n",
    "\n",
    "def get_root(fname):\n",
    "    tree = ET.parse(fname)\n",
    "    return tree.getroot()\n",
    "\n",
    "\n",
    "def split_file(filename):\n",
    "\n",
    "    # open file and throw all rows into a list\n",
    "    doc_list = []\n",
    "    with open (filename, 'r') as f:\n",
    "        for row in f:\n",
    "            doc_list.append(row)\n",
    "    \n",
    "    \n",
    "    # find indices where xml header appears\n",
    "    search_str = '<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n'\n",
    "    indices = [i for i, x in enumerate(doc_list) if x == search_str]\n",
    "    \n",
    "\n",
    "    # make new list with nest list of each document\n",
    "    indices.append(len(doc_list))\n",
    "    s_doc_list = [doc_list[indices[i]:indices[i+1]] for i in range(len(indices)-1)]\n",
    "    \n",
    "    for n in range(len(s_doc_list)):\n",
    "        content = s_doc_list[n]\n",
    "        with open(\"{}-{}\".format(filename, n), 'w') as w:\n",
    "            for string in content:\n",
    "                w.write(string)\n",
    "    \n",
    "    pass\n",
    "\n",
    "split_file(PATENTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    # REQUIREMENTS\n",
    "        # input a .data file (concatenated XML files)\n",
    "        # output four XML docs\n",
    "            # patent.data-0\n",
    "            # patent.data-1\n",
    "            # ...\n",
    "    # EXECUTION\n",
    "        # with-open doc\n",
    "            # throw contents into a list\n",
    "        # split into multiple lists\n",
    "            # REQUIREMENTS: one list must be sliced based on another list\n",
    "            # RULES: list 1 with n items must be sliced at second list:\n",
    "            # COMPUTATION:\n",
    "                # find indices of all xml headers\n",
    "                # indices list appended with length of document\n",
    "                # new list comprises old list cut at i to i+1 from reference list\n",
    "                    # performed one less than the length of the reference list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "str_list = [\"string 1 \", \"string 2\"]\n",
    "\n",
    "with open('newfile.txt', 'w') as w:\n",
    "    for string in str_list:\n",
    "        w.write(string+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bounds': 1,\n",
      " 'member': 3,\n",
      " 'nd': 4,\n",
      " 'node': 20,\n",
      " 'osm': 1,\n",
      " 'relation': 1,\n",
      " 'tag': 7,\n",
      " 'way': 1}\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Your task is to use the iterative parsing to process the map file and\n",
    "find out not only what tags are there, but also how many, to get the\n",
    "feeling on how much of which data you can expect to have in the map.\n",
    "Fill out the count_tags function. It should return a dictionary with the \n",
    "tag name as the key and number of times this tag can be encountered in \n",
    "the map as value.\n",
    "\n",
    "osm_check = {'tag':number_instances,...}\n",
    "\n",
    "Note that your code will be tested with a different data file than the 'example.osm'\n",
    "\"\"\"\n",
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "\n",
    "# REQS:\n",
    "    # make dict\n",
    "# EX:\n",
    "    # REQS:\n",
    "        # make dict\n",
    "    # RULES:\n",
    "        # node must be uniquely identified\n",
    "        # node must be added to a dict and not repeated\n",
    "        # use xml package to make root\n",
    "    # COMP:\n",
    "        # iterate over each line\n",
    "            # if does not contain '/'\n",
    "                # add to dict\n",
    "                    # if exists already, add and increment value by 1\n",
    "                    # else, just add\n",
    "    # DECOMP:\n",
    "        # Iterate over each line, add node name to dict with value 1\n",
    "            # if item is not in dict already, add and make value 1\n",
    "            # if in dict already, increment that key's value by 1\n",
    "    \n",
    "def count_tags(filename):\n",
    "    # create XML tree objet\n",
    "    tree = ET.parse(filename)\n",
    "    \n",
    "    # create root object to parse the XML tree\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    # holds all tags (keys) and counts for each instance of key (value)\n",
    "    tags_dict = {}\n",
    "    \n",
    "    # fills in dict with tags and counts\n",
    "    for element in tree.iter():\n",
    "        if element.tag in tags_dict:\n",
    "            tags_dict[element.tag] += 1\n",
    "        else:\n",
    "            tags_dict[element.tag] = 1\n",
    "            \n",
    "    return tags_dict\n",
    "\n",
    "def test():\n",
    "\n",
    "    tags = count_tags('supporting-files/example.osm')\n",
    "    pprint.pprint(tags)\n",
    "    assert tags == {'bounds': 1,\n",
    "                     'member': 3,\n",
    "                     'nd': 4,\n",
    "                     'node': 20,\n",
    "                     'osm': 1,\n",
    "                     'relation': 1,\n",
    "                     'tag': 7,\n",
    "                     'way': 1}\n",
    "\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
